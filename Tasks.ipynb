{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57a957c",
   "metadata": {},
   "source": [
    "# Part 1: Product Documentation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16818081",
   "metadata": {},
   "source": [
    "# Task 1: Sitemap Extraction\n",
    "\n",
    "**Objective:** Extract document URLs from Snowflake documentation XML sitemaps, handle nested sitemap references, and persist results to the `CANDIDATE_{INITIALS}_SITEMAP_STAGING` table.\n",
    "\n",
    "**Data Sources:**\n",
    "- `https://docs.snowflake.com/en/sitemap.xml`\n",
    "- `https://other-docs.snowflake.com/en/sitemap.xml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Optional\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea79df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; SitemapBot/1.0)\"}\n",
    "\n",
    "#  Visited sitemaps tracking\n",
    "visited_sitemaps: set[str] = set()\n",
    "\n",
    "\n",
    "def get_namespace(root: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Dynamically extract the namespace from the root element.\n",
    "    Handles: namespaced tags like {http://...}sitemapindex, or no namespace.\n",
    "    \"\"\"\n",
    "    if root.tag.startswith(\"{\"):\n",
    "        return root.tag.split(\"}\")[0] + \"}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def fetch_xml(url: str) -> Optional[ET.Element]:\n",
    "    \"\"\"Fetch and parse an XML document from a URL.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        return ET.fromstring(resp.content)\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_local_tag(element: ET.Element) -> str:\n",
    "    \"\"\"Extract local tag name without namespace prefix.\"\"\"\n",
    "    tag = element.tag\n",
    "    return tag.split(\"}\")[-1] if \"}\" in tag else tag\n",
    "\n",
    "\n",
    "def is_sitemap_index(root: ET.Element) -> bool:\n",
    "    \"\"\"Return True if the root element is a <sitemapindex>.\"\"\"\n",
    "    return get_local_tag(root) == \"sitemapindex\"\n",
    "\n",
    "\n",
    "def parse_sitemap(url: str, depth: int = 0) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Recursively parse a sitemap URL.\n",
    "\n",
    "    - Sitemap Index  → record SITEMAP entries + follow each child <sitemap>/<loc> link\n",
    "    - URL Set        → extract each <url> entry (loc, lastmod)\n",
    "\n",
    "    Returns list of dicts with ENTRY_TYPE = 'SITEMAP' or 'URL'\n",
    "    \"\"\"\n",
    "    global visited_sitemaps\n",
    "    indent = \"  \" * depth\n",
    "\n",
    "    #  Skip if already visited (prevents loops)\n",
    "    if url in visited_sitemaps:\n",
    "        print(f\"{indent}[SKIP] Already visited: {url}\")\n",
    "        return []\n",
    "    visited_sitemaps.add(url)\n",
    "\n",
    "    print(f\"{indent}Processing: {url}\")\n",
    "\n",
    "    root = fetch_xml(url)\n",
    "    if root is None:\n",
    "        return []\n",
    "\n",
    "    #  Dynamic namespace extraction\n",
    "    ns = get_namespace(root)\n",
    "    results = []\n",
    "\n",
    "    if is_sitemap_index(root):\n",
    "        # Sitemap Index File\n",
    "        print(f\"{indent}  -> Sitemap Index (contains nested sitemaps)\")\n",
    "        sitemap_entries = root.findall(f\"{ns}sitemap\")\n",
    "        print(f\"{indent}  -> Found {len(sitemap_entries)} child sitemap(s)\")\n",
    "\n",
    "        for sitemap in sitemap_entries:\n",
    "            loc_elem = sitemap.find(f\"{ns}loc\")\n",
    "            lastmod_elem = sitemap.find(f\"{ns}lastmod\")\n",
    "\n",
    "            child_url = loc_elem.text.strip() if loc_elem is not None and loc_elem.text else None\n",
    "            lastmod = lastmod_elem.text.strip() if lastmod_elem is not None and lastmod_elem.text else None\n",
    "\n",
    "            if child_url:\n",
    "                #  Store the SITEMAP entry itself\n",
    "                results.append({\n",
    "                    \"loc\": child_url,\n",
    "                    \"lastmod\": lastmod,\n",
    "                    \"source_sitemap\": url,\n",
    "                    \"entry_type\": \"SITEMAP\",\n",
    "                })\n",
    "                #  Recurse into child sitemap \n",
    "                results.extend(parse_sitemap(child_url, depth + 1))\n",
    "                time.sleep(0.2)  # polite crawling delay\n",
    "    else:\n",
    "        # URL Set File\n",
    "        url_entries = root.findall(f\"{ns}url\")\n",
    "        print(f\"{indent}  -> URL Set with {len(url_entries)} URL(s)\")\n",
    "\n",
    "        for entry in url_entries:\n",
    "            loc_elem = entry.find(f\"{ns}loc\")\n",
    "            lastmod_elem = entry.find(f\"{ns}lastmod\")\n",
    "\n",
    "            loc = loc_elem.text.strip() if loc_elem is not None and loc_elem.text else None\n",
    "            lastmod = lastmod_elem.text.strip() if lastmod_elem is not None and lastmod_elem.text else None\n",
    "\n",
    "            if loc:\n",
    "                results.append({\n",
    "                    \"loc\": loc,\n",
    "                    \"lastmod\": lastmod,\n",
    "                    \"source_sitemap\": url,\n",
    "                    \"entry_type\": \"URL\",\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def reset_visited():\n",
    "    \"\"\"Reset the visited sitemaps set (call before a fresh crawl).\"\"\"\n",
    "    global visited_sitemaps\n",
    "    visited_sitemaps = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a26306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction from: https://docs.snowflake.com/en/sitemap.xml\n",
      "Processing: https://docs.snowflake.com/en/sitemap.xml\n",
      "  -> URL Set with 6617 URL(s)\n",
      "\n",
      "  => Extracted 6617 entries from https://docs.snowflake.com/en/sitemap.xml\n",
      "Starting extraction from: https://other-docs.snowflake.com/en/sitemap.xml\n",
      "Processing: https://other-docs.snowflake.com/en/sitemap.xml\n",
      "  -> URL Set with 3 URL(s)\n",
      "\n",
      "  => Extracted 3 entries from https://other-docs.snowflake.com/en/sitemap.xml\n",
      "Total entries extracted: 6620\n",
      "  SITEMAP entries: 0\n",
      "  URL entries:     6620\n"
     ]
    }
   ],
   "source": [
    "SITEMAP_URLS = [\n",
    "    \"https://docs.snowflake.com/en/sitemap.xml\",\n",
    "    \"https://other-docs.snowflake.com/en/sitemap.xml\",\n",
    "]\n",
    "\n",
    "all_entries: list[dict] = []\n",
    "\n",
    "# Reset visited set before fresh crawl\n",
    "reset_visited()\n",
    "\n",
    "for sitemap_url in SITEMAP_URLS:\n",
    "    print(f\"Starting extraction from: {sitemap_url}\")\n",
    "    entries = parse_sitemap(sitemap_url)\n",
    "    print(f\"\\n  => Extracted {len(entries)} entries from {sitemap_url}\")\n",
    "    all_entries.extend(entries)\n",
    "\n",
    "print(f\"Total entries extracted: {len(all_entries)}\")\n",
    "print(f\"  SITEMAP entries: {sum(1 for e in all_entries if e['entry_type'] == 'SITEMAP')}\")\n",
    "print(f\"  URL entries:     {sum(1 for e in all_entries if e['entry_type'] == 'URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6527247",
   "metadata": {},
   "source": [
    "## Build DataFrame & Deduplicate\n",
    "\n",
    "Create a pandas DataFrame from the extracted URLs, remove duplicates, and add an `extracted_at` timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f86f524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate entries\n",
      "Unique entries remaining: 6620\n",
      "\n",
      "Columns: ['loc', 'lastmod', 'source_sitemap', 'entry_type', 'extracted_at']\n",
      "\n",
      "Entry type breakdown:\n",
      "  SITEMAP: 0\n",
      "  URL:     6620\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>lastmod</th>\n",
       "      <th>source_sitemap</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>extracted_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/api-reference</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://docs.snowflake.com/en/appendices</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 loc lastmod  \\\n",
       "0        https://docs.snowflake.com/en/api-reference    None   \n",
       "1           https://docs.snowflake.com/en/appendices    None   \n",
       "2  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "3  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "4  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "5  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "6  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "7  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "8  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "9  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "\n",
       "                              source_sitemap entry_type         extracted_at  \n",
       "0  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "1  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "2  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "3  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "4  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "5  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "6  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "7  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "8  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  \n",
       "9  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:58  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_entries)\n",
    "\n",
    "# Deduplicate on loc (keep first occurrence)\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates(subset=[\"loc\"], keep=\"first\").reset_index(drop=True)\n",
    "print(f\"Removed {initial_count - len(df)} duplicate entries\")\n",
    "print(f\"Unique entries remaining: {len(df)}\")\n",
    "\n",
    "# Add extraction timestamp\n",
    "df[\"extracted_at\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nEntry type breakdown:\")\n",
    "print(f\"  SITEMAP: {len(df[df['entry_type'] == 'SITEMAP'])}\")\n",
    "print(f\"  URL:     {len(df[df['entry_type'] == 'URL'])}\")\n",
    "print(f\"\\nSample rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a932e",
   "metadata": {},
   "source": [
    "## Storage Configuration\n",
    "\n",
    "Configure the storage layer. `CANDIDATE_{YOUR_INITIALS}_{TABLE_NAME}` format.\n",
    "\n",
    "**Options:**\n",
    "- `USE_SNOWFLAKE = True` → Write to Snowflake (production)\n",
    "- `USE_SNOWFLAKE = False` → Write to local SQLite (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521599ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage mode: SQLite\n",
      "Target table: CANDIDATE_SSP_SITEMAP_STAGING\n"
     ]
    }
   ],
   "source": [
    "#  Storage Configuration\n",
    "\n",
    "#  Set your initials per the spec \n",
    "INITIALS = \"SSP\" \n",
    "\n",
    "USE_SNOWFLAKE = False\n",
    "\n",
    "#  Table naming per spec: CANDIDATE_{INITIALS}_{TABLE_NAME}\n",
    "TABLE_NAME = f\"CANDIDATE_{INITIALS}_SITEMAP_STAGING\"\n",
    "\n",
    "#  SQLite config\n",
    "DB_PATH = \"pipeline.db\"\n",
    "\n",
    "#  Snowflake config  \n",
    "SNOWFLAKE_ACCOUNT = os.environ.get(\"SNOWFLAKE_ACCOUNT\", \"\")\n",
    "SNOWFLAKE_USER = os.environ.get(\"SNOWFLAKE_USER\", \"\")\n",
    "SNOWFLAKE_PASSWORD = os.environ.get(\"SNOWFLAKE_PASSWORD\", \"\")\n",
    "SNOWFLAKE_DATABASE = os.environ.get(\"SNOWFLAKE_DATABASE\", \"\")\n",
    "SNOWFLAKE_SCHEMA = os.environ.get(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n",
    "SNOWFLAKE_WAREHOUSE = os.environ.get(\"SNOWFLAKE_WAREHOUSE\", \"\")\n",
    "\n",
    "print(f\"Storage mode: {'Snowflake' if USE_SNOWFLAKE else 'SQLite'}\")\n",
    "print(f\"Target table: {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeaba6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite table CANDIDATE_SSP_SITEMAP_STAGING is ready.\n"
     ]
    }
   ],
   "source": [
    "#  Connect and Create Staging Table\n",
    "\n",
    "if USE_SNOWFLAKE:\n",
    "    # Snowflake Connection \n",
    "    import snowflake.connector\n",
    "    \n",
    "    conn = snowflake.connector.connect(\n",
    "        account=SNOWFLAKE_ACCOUNT,\n",
    "        user=SNOWFLAKE_USER,\n",
    "        password=SNOWFLAKE_PASSWORD,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    CREATE_STAGING_DDL = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        LOC              VARCHAR(4096) NOT NULL,\n",
    "        LASTMOD          VARCHAR(64),\n",
    "        SOURCE_SITEMAP   VARCHAR(4096),\n",
    "        ENTRY_TYPE       VARCHAR(16),\n",
    "        EXTRACTED_AT     TIMESTAMP_NTZ\n",
    "    )\n",
    "    \"\"\"\n",
    "    cur.execute(CREATE_STAGING_DDL)\n",
    "    print(f\"Snowflake table {TABLE_NAME} is ready.\")\n",
    "    \n",
    "else:\n",
    "    # SQLite Connection\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Drop and recreate to ensure schema matches\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "    \n",
    "    CREATE_STAGING_DDL = f\"\"\"\n",
    "    CREATE TABLE {TABLE_NAME} (\n",
    "        LOC              TEXT NOT NULL,\n",
    "        LASTMOD          TEXT,\n",
    "        SOURCE_SITEMAP   TEXT,\n",
    "        ENTRY_TYPE       TEXT,\n",
    "        EXTRACTED_AT     TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    cur.execute(CREATE_STAGING_DDL)   \n",
    "    print(f\"SQLite table {TABLE_NAME} is ready.\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5a859",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Rename columns to match the table schema (uppercase), clear existing data for idempotent reloads, then bulk-insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a06f0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared CANDIDATE_SSP_SITEMAP_STAGING\n",
      "\n",
      "Load complete: 6620 rows inserted into CANDIDATE_SSP_SITEMAP_STAGING (SQLite)\n"
     ]
    }
   ],
   "source": [
    "df_upload = df.rename(columns={\n",
    "    \"loc\":            \"LOC\",\n",
    "    \"lastmod\":        \"LASTMOD\",\n",
    "    \"source_sitemap\": \"SOURCE_SITEMAP\",\n",
    "    \"entry_type\":     \"ENTRY_TYPE\",\n",
    "    \"extracted_at\":   \"EXTRACTED_AT\",\n",
    "})\n",
    "\n",
    "# Delete existing rows for idempotent reload\n",
    "if USE_SNOWFLAKE:\n",
    "    cur.execute(f\"DELETE FROM {TABLE_NAME}\")\n",
    "    print(f\"Cleared {TABLE_NAME}\")\n",
    "    \n",
    "    # Snowflake bulk insert via write_pandas\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    \n",
    "    success, nchunks, nrows, _ = write_pandas(\n",
    "        conn, df_upload, TABLE_NAME,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False,\n",
    "    )\n",
    "    print(f\"\\nLoad complete: {nrows} rows inserted into {TABLE_NAME} (Snowflake)\")\n",
    "    \n",
    "else:\n",
    "    cur.execute(f\"DELETE FROM {TABLE_NAME}\")\n",
    "    conn.commit()\n",
    "    print(f\"Cleared {TABLE_NAME}\")\n",
    "    \n",
    "    # SQLite bulk load\n",
    "    df_upload.to_sql(TABLE_NAME, conn, if_exists=\"append\", index=False)\n",
    "    print(f\"\\nLoad complete: {len(df_upload)} rows inserted into {TABLE_NAME} (SQLite)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6304f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_SITEMAP</th>\n",
       "      <th>ENTRY_TYPE</th>\n",
       "      <th>ENTRY_COUNT</th>\n",
       "      <th>WITH_LASTMOD</th>\n",
       "      <th>WITHOUT_LASTMOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>6617</td>\n",
       "      <td>0</td>\n",
       "      <td>6617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://other-docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    SOURCE_SITEMAP ENTRY_TYPE  ENTRY_COUNT  \\\n",
       "0        https://docs.snowflake.com/en/sitemap.xml        URL         6617   \n",
       "1  https://other-docs.snowflake.com/en/sitemap.xml        URL            3   \n",
       "\n",
       "   WITH_LASTMOD  WITHOUT_LASTMOD  \n",
       "0             0             6617  \n",
       "1             0                3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary by source sitemap and entry type\n",
    "df_summary = pd.read_sql(f\"\"\"\n",
    "    SELECT SOURCE_SITEMAP,\n",
    "           ENTRY_TYPE,\n",
    "           COUNT(*)                                                AS ENTRY_COUNT,\n",
    "           SUM(CASE WHEN LASTMOD IS NOT NULL THEN 1 ELSE 0 END)   AS WITH_LASTMOD,\n",
    "           SUM(CASE WHEN LASTMOD IS NULL     THEN 1 ELSE 0 END)   AS WITHOUT_LASTMOD\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY SOURCE_SITEMAP, ENTRY_TYPE\n",
    "    ORDER BY SOURCE_SITEMAP, ENTRY_TYPE\n",
    "\"\"\", conn)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce561e",
   "metadata": {},
   "source": [
    "# Task 2: Data Consolidation\n",
    "\n",
    "**Objective:** Consolidate extracted entries from `CANDIDATE_{INITIALS}_SITEMAP_STAGING` into a slowly-changing **observation table** `CANDIDATE_{INITIALS}_DOCS_MASTER`.\n",
    "\n",
    "DOCS_MASTER is not a content table — it models the pipeline's observation of sitemap URLs over time. Each row represents a unique documentation URL; temporal fields record **when the pipeline first discovered it** and **the most recent run in which it was still present** in any sitemap.\n",
    "\n",
    "- **Source aggregation** - URLs appearing in multiple sitemaps accumulate sources (comma-separated, never overwritten)\n",
    "- **Pipeline temporal tracking** - `FIRST_SEEN_AT` = first pipeline run that discovered this URL; `LAST_SEEN_AT` = most recent run where the URL still existed in a sitemap\n",
    "\n",
    "- **Idempotent UPSERT** - re-runs update `LAST_SEEN_AT` and merge sources, never duplicate rows- **Batch processing** — handles datasets exceeding 10,000 rows via chunked inserts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08f739",
   "metadata": {},
   "source": [
    "## Create the DOCS_MASTER Table\n",
    "\n",
    "Schema uses `LOC` as the primary key (one row per URL — no duplicates). `SOURCES` accumulates every sitemap in which the URL has been observed (comma-separated).\n",
    "\n",
    "| Field | Role | Description |\n",
    "|---|---|---|\n",
    "| `LASTMOD` | **Optional upstream metadata** | Value from the sitemap's `<lastmod>` tag. Stored as-is; NULL when the sitemap omits it. Never fabricated or backfilled. |\n",
    "| `FIRST_SEEN_AT` | **Pipeline observation** | UTC timestamp of the first pipeline run that discovered this URL in any sitemap |\n",
    "| `LAST_SEEN_AT` | **Pipeline observation** | UTC timestamp of the most recent pipeline run where the URL was still present |\n",
    "\n",
    "If a URL disappears from all sitemaps, `LAST_SEEN_AT` stops advancing — enabling sitemap churn and coverage-stability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a37121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table CANDIDATE_SSP_DOCS_MASTER is ready.\n"
     ]
    }
   ],
   "source": [
    "MASTER_TABLE = f\"CANDIDATE_{INITIALS}_DOCS_MASTER\"\n",
    "STAGING_TABLE = f\"CANDIDATE_{INITIALS}_SITEMAP_STAGING\"\n",
    "\n",
    "# Drop and recreate to ensure schema matches\n",
    "cur.execute(f\"DROP TABLE IF EXISTS {MASTER_TABLE}\")\n",
    "\n",
    "CREATE_MASTER_DDL = f\"\"\"\n",
    "CREATE TABLE {MASTER_TABLE} (\n",
    "    LOC            TEXT NOT NULL PRIMARY KEY,\n",
    "    LASTMOD        TEXT,\n",
    "    SOURCES        TEXT,\n",
    "    ENTRY_TYPE     TEXT,\n",
    "    FIRST_SEEN_AT  TEXT NOT NULL,\n",
    "    LAST_SEEN_AT   TEXT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(CREATE_MASTER_DDL)\n",
    "conn.commit()\n",
    "print(f\"Table {MASTER_TABLE} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f427a",
   "metadata": {},
   "source": [
    "## Batch Consolidation (UPSERT)\n",
    "\n",
    "**Idempotency strategy:** SQLite `INSERT … ON CONFLICT` (UPSERT) — a lightweight SCD (slowly-changing dimension) pattern.\n",
    "\n",
    "| Scenario | `FIRST_SEEN_AT` | `LAST_SEEN_AT` | `SOURCES` |\n",
    "|---|---|---|---|\n",
    "| **New URL** (first observation) | Set to current run timestamp | Set to current run timestamp | Set to this sitemap |\n",
    "| **Existing URL** (re-observed) | **Preserved** (never overwritten) | **Bumped** to current run timestamp | Appended if source is new |\n",
    "| **URL removed from sitemap** | Preserved | Unchanged (stops advancing) | Preserved |\n",
    "\n",
    "Processing is **batched** (configurable `BATCH_SIZE`, default 5000) to handle datasets exceeding 10,000 rows without memory pressure.\n",
    "\n",
    "**Source accumulation** — URL appears in source A on Monday, then source B on Tuesday → `SOURCES = \"A,B\"` (merge, not overwrite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b2635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging rows to consolidate: 6620\n",
      "  Batch 1: rows 1–5000\n",
      " Inserted=5000, updated=0\n",
      "  Batch 2: rows 5001–6620\n",
      " Inserted=1620, updated=0\n",
      "\n",
      "Consolidation complete: 6620 inserted, 0 updated\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Read staging data in chunks\n",
    "staging_df = pd.read_sql(f\"SELECT * FROM {STAGING_TABLE}\", conn)\n",
    "total_rows = len(staging_df)\n",
    "print(f\"Staging rows to consolidate: {total_rows}\")\n",
    "\n",
    "now = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#  UPSERT SQL — pipeline observation merge\n",
    "# On conflict (same LOC already observed in a prior run):\n",
    "#  1. LASTMOD       - COALESCE: preserve upstream sitemap metadata as-is;  NULL is the correct value when the sitemap omits <lastmod>\n",
    "#  2. SOURCES       - append new source if not already present (merge, not overwrite)\n",
    "#  3. ENTRY_TYPE    - preserve from first observation\n",
    "#  4. FIRST_SEEN_AT - NOT in UPDATE SET - frozen at original pipeline discovery time\n",
    "#  5. LAST_SEEN_AT  - bumped to current run timestamp (pipeline observation, NOT content update)\n",
    "\n",
    "UPSERT_SQL = f\"\"\"\n",
    "INSERT INTO {MASTER_TABLE} (LOC, LASTMOD, SOURCES, ENTRY_TYPE, FIRST_SEEN_AT, LAST_SEEN_AT)\n",
    "VALUES (?, ?, ?, ?, ?, ?)\n",
    "ON CONFLICT(LOC) DO UPDATE SET\n",
    "    LASTMOD      = COALESCE(excluded.LASTMOD, {MASTER_TABLE}.LASTMOD),\n",
    "    SOURCES      = CASE\n",
    "                     WHEN ',' || {MASTER_TABLE}.SOURCES || ',' LIKE '%,' || excluded.SOURCES || ',%'\n",
    "                       THEN {MASTER_TABLE}.SOURCES\n",
    "                     ELSE {MASTER_TABLE}.SOURCES || ',' || excluded.SOURCES\n",
    "                   END,\n",
    "    LAST_SEEN_AT = excluded.LAST_SEEN_AT\n",
    "\"\"\"\n",
    "\n",
    "# Process in batches\n",
    "inserted = 0\n",
    "updated = 0\n",
    "\n",
    "for batch_start in range(0, total_rows, BATCH_SIZE):\n",
    "    batch = staging_df.iloc[batch_start : batch_start + BATCH_SIZE]\n",
    "    batch_num = batch_start // BATCH_SIZE + 1\n",
    "    print(f\"  Batch {batch_num}: rows {batch_start + 1}–{min(batch_start + BATCH_SIZE, total_rows)}\")\n",
    "\n",
    "    rows = [\n",
    "        (\n",
    "            row[\"LOC\"],\n",
    "            row[\"LASTMOD\"] if pd.notna(row[\"LASTMOD\"]) else None,\n",
    "            row[\"SOURCE_SITEMAP\"],\n",
    "            row[\"ENTRY_TYPE\"] if pd.notna(row.get(\"ENTRY_TYPE\")) else \"URL\",\n",
    "            now,\n",
    "            now,\n",
    "        )\n",
    "        for _, row in batch.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Count existing rows before upsert to track inserts vs updates\n",
    "    locs = [r[0] for r in rows]\n",
    "    placeholders = \",\".join(\"?\" * len(locs))\n",
    "    existing = set(\n",
    "        r[0]\n",
    "        for r in cur.execute(\n",
    "            f\"SELECT LOC FROM {MASTER_TABLE} WHERE LOC IN ({placeholders})\", locs\n",
    "        ).fetchall()\n",
    "    )\n",
    "\n",
    "    cur.executemany(UPSERT_SQL, rows)\n",
    "    conn.commit()\n",
    "\n",
    "    batch_updated = len(existing)\n",
    "    batch_inserted = len(rows) - batch_updated\n",
    "    inserted += batch_inserted\n",
    "    updated += batch_updated\n",
    "\n",
    "    print(f\" Inserted={batch_inserted}, updated={batch_updated}\")\n",
    "\n",
    "print(f\"\\nConsolidation complete: {inserted} inserted, {updated} updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff9895",
   "metadata": {},
   "source": [
    "## Verify Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fc0912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in CANDIDATE_SSP_DOCS_MASTER: 6620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>LASTMOD</th>\n",
       "      <th>SOURCES</th>\n",
       "      <th>ENTRY_TYPE</th>\n",
       "      <th>FIRST_SEEN_AT</th>\n",
       "      <th>LAST_SEEN_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/api-reference</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://docs.snowflake.com/en/appendices</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>URL</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 LOC LASTMOD  \\\n",
       "0        https://docs.snowflake.com/en/api-reference    None   \n",
       "1           https://docs.snowflake.com/en/appendices    None   \n",
       "2  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "3  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "4  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "5  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "6  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "7  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "8  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "9  https://docs.snowflake.com/en/collaboration/co...    None   \n",
       "\n",
       "                                     SOURCES ENTRY_TYPE        FIRST_SEEN_AT  \\\n",
       "0  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "1  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "2  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "3  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "4  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "5  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "6  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "7  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "8  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "9  https://docs.snowflake.com/en/sitemap.xml        URL  2026-02-09 05:30:59   \n",
       "\n",
       "          LAST_SEEN_AT  \n",
       "0  2026-02-09 05:30:59  \n",
       "1  2026-02-09 05:30:59  \n",
       "2  2026-02-09 05:30:59  \n",
       "3  2026-02-09 05:30:59  \n",
       "4  2026-02-09 05:30:59  \n",
       "5  2026-02-09 05:30:59  \n",
       "6  2026-02-09 05:30:59  \n",
       "7  2026-02-09 05:30:59  \n",
       "8  2026-02-09 05:30:59  \n",
       "9  2026-02-09 05:30:59  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row count\n",
    "master_count = cur.execute(f\"SELECT COUNT(*) FROM {MASTER_TABLE}\").fetchone()[0]\n",
    "print(f\"Rows in {MASTER_TABLE}: {master_count}\")\n",
    "\n",
    "# Sample rows\n",
    "df_master_sample = pd.read_sql(f\"SELECT * FROM {MASTER_TABLE} LIMIT 10\", conn)\n",
    "df_master_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd92221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs by number of distinct sources:\n",
      " NUM_SOURCES  URL_COUNT\n",
      "           1       6620\n"
     ]
    }
   ],
   "source": [
    "df_source_dist = pd.read_sql(f\"\"\"\n",
    "    SELECT\n",
    "        LENGTH(SOURCES) - LENGTH(REPLACE(SOURCES, ',', '')) + 1 AS NUM_SOURCES,\n",
    "        COUNT(*) AS URL_COUNT\n",
    "    FROM {MASTER_TABLE}\n",
    "    GROUP BY NUM_SOURCES\n",
    "    ORDER BY NUM_SOURCES\n",
    "\"\"\", conn)\n",
    "print(\"URLs by number of distinct sources:\")\n",
    "print(df_source_dist.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b29aa6",
   "metadata": {},
   "source": [
    "## Idempotency Test\n",
    "\n",
    "Simulate a second pipeline run: re-execute the UPSERT over all staging rows. This proves:\n",
    "\n",
    "1. **No duplicates** — row count stays the same3. **`LAST_SEEN_AT` bumped** — advances to the new run timestamp, proving temporal tracking works across runs\n",
    "2. **`FIRST_SEEN_AT` preserved** — original discovery timestamp is never overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f226e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before re-run: 6620\n",
      "Rows after  re-run: 6620\n",
      "Duplicates created: 0\n",
      "Idempotency: no duplicates created on re-run\n",
      "\n",
      "Pipeline observation timestamps after second run:\n",
      "                                                                     LOC       FIRST_SEEN_AT        LAST_SEEN_AT\n",
      "                             https://docs.snowflake.com/en/api-reference 2026-02-09 05:30:59 2026-02-09 05:30:59\n",
      "                                https://docs.snowflake.com/en/appendices 2026-02-09 05:30:59 2026-02-09 05:30:59\n",
      "https://docs.snowflake.com/en/collaboration/collaboration-listings-about 2026-02-09 05:30:59 2026-02-09 05:30:59\n",
      "\n",
      "Temporal tracking: 0 URLs have FIRST_SEEN_AT ≠ LAST_SEEN_AT\n",
      "  (FIRST_SEEN_AT frozen at Run 1; LAST_SEEN_AT advanced to Run 2)\n"
     ]
    }
   ],
   "source": [
    "count_before = cur.execute(f\"SELECT COUNT(*) FROM {MASTER_TABLE}\").fetchone()[0]\n",
    "\n",
    "# Simulate a second pipeline run — same staging data, new timestamp\n",
    "now_rerun = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "for batch_start in range(0, total_rows, BATCH_SIZE):\n",
    "    batch = staging_df.iloc[batch_start : batch_start + BATCH_SIZE]\n",
    "    rows = [\n",
    "        (\n",
    "            row[\"LOC\"],\n",
    "            row[\"LASTMOD\"] if pd.notna(row[\"LASTMOD\"]) else None,\n",
    "            row[\"SOURCE_SITEMAP\"],\n",
    "            row[\"ENTRY_TYPE\"] if pd.notna(row.get(\"ENTRY_TYPE\")) else \"URL\",\n",
    "            now_rerun,\n",
    "            now_rerun,\n",
    "        )\n",
    "        for _, row in batch.iterrows()\n",
    "    ]\n",
    "    cur.executemany(UPSERT_SQL, rows)\n",
    "conn.commit()\n",
    "\n",
    "count_after = cur.execute(f\"SELECT COUNT(*) FROM {MASTER_TABLE}\").fetchone()[0]\n",
    "\n",
    "# 1. No duplicates\n",
    "print(f\"Rows before re-run: {count_before}\")\n",
    "print(f\"Rows after  re-run: {count_after}\")\n",
    "print(f\"Duplicates created: {count_after - count_before}\")\n",
    "assert count_before == count_after, \"IDEMPOTENCY VIOLATED — duplicate rows detected!\"\n",
    "print(\"Idempotency: no duplicates created on re-run\")\n",
    "\n",
    "# 2. FIRST_SEEN_AT preserved, LAST_SEEN_AT bumped\n",
    "sample = pd.read_sql(f\"\"\"\n",
    "    SELECT LOC, FIRST_SEEN_AT, LAST_SEEN_AT\n",
    "    FROM {MASTER_TABLE}\n",
    "    WHERE LAST_SEEN_AT = '{now_rerun}'\n",
    "    LIMIT 3\n",
    "\"\"\", conn)\n",
    "print(f\"\\nPipeline observation timestamps after second run:\")\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "# Verify FIRST_SEEN_AT != LAST_SEEN_AT (proves original discovery preserved)\n",
    "diverged = cur.execute(f\"\"\"\n",
    "    SELECT COUNT(*) FROM {MASTER_TABLE}\n",
    "    WHERE FIRST_SEEN_AT != LAST_SEEN_AT\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"\\nTemporal tracking: {diverged} URLs have FIRST_SEEN_AT ≠ LAST_SEEN_AT\")\n",
    "print(f\"  (FIRST_SEEN_AT frozen at Run 1; LAST_SEEN_AT advanced to Run 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc7755",
   "metadata": {},
   "source": [
    "# Task 3: Document Content Ingestion\n",
    "\n",
    "**Objective:** Fetch and store document content from URLs in `CANDIDATE_SSP_DOCS_MASTER`.\n",
    "\n",
    "**Target table:** `CANDIDATE_SSP_DOCUMENT_CONTENT`\n",
    "\n",
    "**Key Features:**\n",
    "- **Throttling** — per-request delay + exponential backoff on transient failures\n",
    "- **Retry with backoff** — up to 3 retries per URL with exponential wait\n",
    "- **Content change detection** — SHA-256 hash comparison; skip fetch if content unchanged\n",
    "- **Large document handling** — streaming download, truncate at 5 MB\n",
    "- **Consistent-timeout circuit breaker** — after 5 consecutive failures on a URL, mark it as `circuit_broken`\n",
    "- **Batch processing** — configurable batch size for 50K+ URL scalability\n",
    "- **Idempotent** — only fetches URLs that are new or have stale content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bb551",
   "metadata": {},
   "source": [
    "## Create DOCUMENT_CONTENT Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3f7b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table CANDIDATE_SSP_DOCUMENT_CONTENT is ready.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CONTENT_TABLE = \"CANDIDATE_SSP_DOCUMENT_CONTENT\"\n",
    "\n",
    "CREATE_CONTENT_DDL = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CONTENT_TABLE} (\n",
    "    LOC                   TEXT NOT NULL PRIMARY KEY,\n",
    "    CONTENT               TEXT,\n",
    "    CONTENT_HASH          TEXT,\n",
    "    CONTENT_SIZE_BYTES    INTEGER,\n",
    "    HTTP_STATUS           INTEGER,\n",
    "    FETCH_STATUS          TEXT,\n",
    "    RETRY_COUNT           INTEGER DEFAULT 0,\n",
    "    CONSECUTIVE_FAILURES  INTEGER DEFAULT 0,\n",
    "    FIRST_FETCHED_AT      TEXT,\n",
    "    LAST_FETCHED_AT       TEXT,\n",
    "    LAST_SUCCESS_AT       TEXT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(CREATE_CONTENT_DDL)\n",
    "conn.commit()\n",
    "print(f\"Table {CONTENT_TABLE} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c41d8",
   "metadata": {},
   "source": [
    "## Throttled Document Fetcher\n",
    "\n",
    "Single-URL fetch function with:\n",
    "- **Streaming download** — reads in 64 KB chunks; truncates at `MAX_CONTENT_SIZE` (5 MB) to handle large docs\n",
    "- **Exponential backoff** — retries on transient HTTP codes (408, 429, 500–504) and timeouts\n",
    "- **SHA-256 hashing** — for content change detection\n",
    "- **Per-request delay** — `THROTTLE_DELAY` seconds between requests to respect source servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67f60aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document fetcher ready.\n",
      "  Batch size: 500 | Workers: 5 | Timeout: 30s | Max retries: 3\n"
     ]
    }
   ],
   "source": [
    "# Throttle / retry constants\n",
    "FETCH_BATCH_SIZE       = 500       # URLs per processing batch\n",
    "MAX_WORKERS            = 5         # concurrent threads\n",
    "REQUEST_TIMEOUT        = 30        # seconds per HTTP request\n",
    "MAX_RETRIES            = 3         # retry attempts on transient errors\n",
    "BACKOFF_BASE           = 2         # exponential backoff multiplier\n",
    "THROTTLE_DELAY         = 0.3       # seconds between successive requests\n",
    "MAX_CONTENT_SIZE       = 5 * 1024 * 1024   # 5 MB — truncate beyond this\n",
    "MAX_CONSECUTIVE_FAILS  = 5         # circuit-break after N consecutive failures\n",
    "TRANSIENT_STATUS_CODES = {408, 429, 500, 502, 503, 504}\n",
    "\n",
    "FETCH_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DocIngestionBot/1.0)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "\n",
    "def compute_hash(content: str) -> str:\n",
    "    \"\"\"SHA-256 hash for content change detection.\"\"\"\n",
    "    return hashlib.sha256(content.encode(\"utf-8\", errors=\"replace\")).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_document(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch a single document with streaming, retries, and throttling.\n",
    "\n",
    "    Returns dict with: content, content_hash, content_size_bytes, http_status, fetch_status, retry_count\n",
    "    \"\"\"\n",
    "    last_exception = None\n",
    "    http_status = None\n",
    "\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                wait = BACKOFF_BASE ** attempt\n",
    "                time.sleep(wait)\n",
    "\n",
    "            with requests.get(url, headers=FETCH_HEADERS,\n",
    "                              timeout=REQUEST_TIMEOUT, stream=True) as resp:\n",
    "                http_status = resp.status_code\n",
    "\n",
    "                # Non-transient HTTP error -> fail immediately\n",
    "                if resp.status_code >= 400 and resp.status_code not in TRANSIENT_STATUS_CODES:\n",
    "                    return {\n",
    "                        \"content\": None, \"content_hash\": None,\n",
    "                        \"content_size_bytes\": 0, \"http_status\": http_status,\n",
    "                        \"fetch_status\": \"failed\", \"retry_count\": attempt,\n",
    "                    }\n",
    "\n",
    "                # Transient error -> retry\n",
    "                if resp.status_code in TRANSIENT_STATUS_CODES:\n",
    "                    last_exception = f\"HTTP {resp.status_code}\"\n",
    "                    continue\n",
    "\n",
    "                # Stream content in chunks, enforce size cap\n",
    "                chunks = []\n",
    "                total_size = 0\n",
    "                truncated = False\n",
    "                for chunk in resp.iter_content(chunk_size=64 * 1024,\n",
    "                                               decode_unicode=True):\n",
    "                    if chunk:\n",
    "                        total_size += len(chunk.encode(\"utf-8\", errors=\"replace\"))\n",
    "                        if total_size <= MAX_CONTENT_SIZE:\n",
    "                            chunks.append(chunk)\n",
    "                        else:\n",
    "                            truncated = True\n",
    "                            break\n",
    "\n",
    "                content = \"\".join(chunks)\n",
    "                if truncated:\n",
    "                    content += f\"\\n\\n[TRUNCATED at {MAX_CONTENT_SIZE/(1024*1024):.0f} MB]\"\n",
    "                    total_size = len(content.encode(\"utf-8\", errors=\"replace\"))\n",
    "\n",
    "                content_hash = compute_hash(content)\n",
    "                time.sleep(THROTTLE_DELAY)\n",
    "\n",
    "                return {\n",
    "                    \"content\": content, \"content_hash\": content_hash,\n",
    "                    \"content_size_bytes\": total_size, \"http_status\": http_status,\n",
    "                    \"fetch_status\": \"success\", \"retry_count\": attempt,\n",
    "                }\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            last_exception = \"timeout\"\n",
    "            http_status = None\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            last_exception = f\"connection_error: {e}\"\n",
    "            http_status = None\n",
    "        except Exception as e:\n",
    "            last_exception = str(e)\n",
    "            http_status = None\n",
    "\n",
    "    is_timeout = \"timeout\" in str(last_exception).lower()\n",
    "    return {\n",
    "        \"content\": None, \"content_hash\": None,\n",
    "        \"content_size_bytes\": 0, \"http_status\": http_status,\n",
    "        \"fetch_status\": \"timeout\" if is_timeout else \"failed\",\n",
    "        \"retry_count\": MAX_RETRIES,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Document fetcher ready.\")\n",
    "print(f\"  Batch size: {FETCH_BATCH_SIZE} | Workers: {MAX_WORKERS} | \"\n",
    "      f\"Timeout: {REQUEST_TIMEOUT}s | Max retries: {MAX_RETRIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68732712",
   "metadata": {},
   "source": [
    "## Batch Ingestion Engine\n",
    "\n",
    "Processes URLs from `DOCS_MASTER` in batches of `FETCH_BATCH_SIZE`:\n",
    "\n",
    "1. **Skip unchanged** - if the URL already exists in `DOCUMENT_CONTENT` with a content hash and `LASTMOD` hasn't changed then skip (no re-fetch)\n",
    "2. **Skip circuit-broken** - URLs with ≥ `MAX_CONSECUTIVE_FAILS` consecutive failures are skipped\n",
    "3. **Concurrent fetch** - uses a `ThreadPoolExecutor` (capped at `MAX_WORKERS`) for parallelism\n",
    "4. **UPSERT results** - `INSERT … ON CONFLICT` updates content, hash, status, and timestamps\n",
    "5. **Per-batch progress** - prints running totals for success / failed / skipped / unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed07070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSERT SQL for DOCUMENT_CONTENT\n",
    "CONTENT_UPSERT_SQL = f\"\"\"\n",
    "INSERT INTO {CONTENT_TABLE}\n",
    "    (LOC, CONTENT, CONTENT_HASH, CONTENT_SIZE_BYTES, HTTP_STATUS,\n",
    "     FETCH_STATUS, RETRY_COUNT, CONSECUTIVE_FAILURES,\n",
    "     FIRST_FETCHED_AT, LAST_FETCHED_AT, LAST_SUCCESS_AT)\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "ON CONFLICT(LOC) DO UPDATE SET\n",
    "    CONTENT              = excluded.CONTENT,\n",
    "    CONTENT_HASH         = excluded.CONTENT_HASH,\n",
    "    CONTENT_SIZE_BYTES   = excluded.CONTENT_SIZE_BYTES,\n",
    "    HTTP_STATUS          = excluded.HTTP_STATUS,\n",
    "    FETCH_STATUS         = excluded.FETCH_STATUS,\n",
    "    RETRY_COUNT          = excluded.RETRY_COUNT,\n",
    "    CONSECUTIVE_FAILURES = excluded.CONSECUTIVE_FAILURES,\n",
    "    LAST_FETCHED_AT      = excluded.LAST_FETCHED_AT,\n",
    "    LAST_SUCCESS_AT      = COALESCE(excluded.LAST_SUCCESS_AT, {CONTENT_TABLE}.LAST_SUCCESS_AT)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_existing_content_info(cur, locs: list[str]) -> dict:\n",
    "    \"\"\"Return {loc: (content_hash, consecutive_failures)} for URLs already in DOCUMENT_CONTENT.\"\"\"\n",
    "    if not locs:\n",
    "        return {}\n",
    "    placeholders = \",\".join(\"?\" * len(locs))\n",
    "    rows = cur.execute(\n",
    "        f\"SELECT LOC, CONTENT_HASH, CONSECUTIVE_FAILURES FROM {CONTENT_TABLE} WHERE LOC IN ({placeholders})\",\n",
    "        locs,\n",
    "    ).fetchall()\n",
    "    return {r[0]: (r[1], r[2]) for r in rows}\n",
    "\n",
    "\n",
    "def get_lastmod_map(cur, locs: list[str]) -> dict:\n",
    "    \"\"\"Return {loc: lastmod} from DOCS_MASTER for change detection.\"\"\"\n",
    "    if not locs:\n",
    "        return {}\n",
    "    placeholders = \",\".join(\"?\" * len(locs))\n",
    "    rows = cur.execute(\n",
    "        f\"SELECT LOC, LASTMOD FROM {MASTER_TABLE} WHERE LOC IN ({placeholders})\",\n",
    "        locs,\n",
    "    ).fetchall()\n",
    "    return {r[0]: r[1] for r in rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e0be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate URLs from DOCS_MASTER: 20\n",
      "\n",
      " Batch 1: 20 URLs (rows 1–20) \n",
      " To fetch: 20 | Skipped (unchanged): 0 | Skipped (circuit): 0\n",
      "  Batch 1 done — success: 20, failed: 0, timeout: 0\n",
      "Ingestion complete.\n",
      "  Success:            20\n",
      "  Failed:             0\n",
      "  Timeout:            0\n",
      "  Skipped (unchanged):0\n",
      "  Skipped (circuit):  0\n"
     ]
    }
   ],
   "source": [
    "#  Configuration: limit how many URLs to fetch in this run \n",
    "\n",
    "FETCH_LIMIT = 20   # change to None for full ingestion\n",
    "\n",
    "#  Load candidate URLs from DOCS_MASTER \n",
    "limit_clause = f\"LIMIT {FETCH_LIMIT}\" if FETCH_LIMIT else \"\"\n",
    "master_urls = pd.read_sql(\n",
    "    f\"SELECT LOC, LASTMOD FROM {MASTER_TABLE} {limit_clause}\", conn\n",
    ")\n",
    "total_candidates = len(master_urls)\n",
    "print(f\"Candidate URLs from DOCS_MASTER: {total_candidates}\")\n",
    "\n",
    "#  Counters \n",
    "stats = {\"success\": 0, \"failed\": 0, \"timeout\": 0,\n",
    "         \"skipped_unchanged\": 0, \"skipped_circuit\": 0}\n",
    "\n",
    "now_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#  Process in batches \n",
    "for batch_start in range(0, total_candidates, FETCH_BATCH_SIZE):\n",
    "    batch_df = master_urls.iloc[batch_start : batch_start + FETCH_BATCH_SIZE]\n",
    "    batch_locs = batch_df[\"LOC\"].tolist()\n",
    "    batch_num = batch_start // FETCH_BATCH_SIZE + 1\n",
    "    print(f\"\\n Batch {batch_num}: {len(batch_locs)} URLs \"\n",
    "          f\"(rows {batch_start+1}–{batch_start+len(batch_locs)}) \")\n",
    "\n",
    "    # Look up existing content info\n",
    "    existing_info = get_existing_content_info(cur, batch_locs)\n",
    "\n",
    "    # Determine which URLs actually need fetching\n",
    "    urls_to_fetch = []\n",
    "    for _, row in batch_df.iterrows():\n",
    "        loc = row[\"LOC\"]\n",
    "        lastmod = row[\"LASTMOD\"]\n",
    "\n",
    "        if loc in existing_info:\n",
    "            prev_hash, consec_fails = existing_info[loc]\n",
    "\n",
    "            # Circuit breaker — skip URLs that consistently fail\n",
    "            if consec_fails >= MAX_CONSECUTIVE_FAILS:\n",
    "                stats[\"skipped_circuit\"] += 1\n",
    "                continue\n",
    "\n",
    "            # Content change detection via upstream LASTMOD metadata:\n",
    "            #   • LASTMOD IS NOT NULL → upstream provides a freshness signal.\n",
    "            #     If we already have content (prev_hash), trust the signal and skip.\n",
    "            #   • LASTMOD IS NULL → no upstream signal (sitemap omits <lastmod>).\n",
    "            #     We cannot know if content changed → must re-fetch.\n",
    "            if prev_hash is not None and lastmod is not None:\n",
    "                stats[\"skipped_unchanged\"] += 1\n",
    "                continue\n",
    "\n",
    "        urls_to_fetch.append(loc)\n",
    "\n",
    "    print(f\" To fetch: {len(urls_to_fetch)} | \"\n",
    "          f\"Skipped (unchanged): {stats['skipped_unchanged']} | \"\n",
    "          f\"Skipped (circuit): {stats['skipped_circuit']}\")\n",
    "\n",
    "    if not urls_to_fetch:\n",
    "        continue\n",
    "\n",
    "    # Concurrent fetch with ThreadPoolExecutor\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "        future_map = {pool.submit(fetch_document, url): url\n",
    "                      for url in urls_to_fetch}\n",
    "        for future in as_completed(future_map):\n",
    "            url = future_map[future]\n",
    "            try:\n",
    "                results[url] = future.result()\n",
    "            except Exception as e:\n",
    "                results[url] = {\n",
    "                    \"content\": None, \"content_hash\": None,\n",
    "                    \"content_size_bytes\": 0, \"http_status\": None,\n",
    "                    \"fetch_status\": \"failed\", \"retry_count\": MAX_RETRIES,\n",
    "                }\n",
    "\n",
    "    # Persist results via UPSERT\n",
    "    upsert_rows = []\n",
    "    for url, res in results.items():\n",
    "        prev = existing_info.get(url)\n",
    "        consec = prev[1] if prev else 0\n",
    "\n",
    "        if res[\"fetch_status\"] == \"success\":\n",
    "            # Reset consecutive failures on success\n",
    "            consec = 0\n",
    "            stats[\"success\"] += 1\n",
    "            last_success = now_ts\n",
    "        else:\n",
    "            consec += 1\n",
    "            stats[res[\"fetch_status\"]] = stats.get(res[\"fetch_status\"], 0) + 1\n",
    "            last_success = None \n",
    "\n",
    "        first_fetched = now_ts if url not in existing_info else None  # keep orig\n",
    "\n",
    "        upsert_rows.append((\n",
    "            url,\n",
    "            res[\"content\"],\n",
    "            res[\"content_hash\"],\n",
    "            res[\"content_size_bytes\"],\n",
    "            res[\"http_status\"],\n",
    "            res[\"fetch_status\"],\n",
    "            res[\"retry_count\"],\n",
    "            consec,\n",
    "            first_fetched if first_fetched else now_ts,\n",
    "            now_ts,\n",
    "            last_success,\n",
    "        ))\n",
    "\n",
    "    cur.executemany(CONTENT_UPSERT_SQL, upsert_rows)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"  Batch {batch_num} done — success: {stats['success']}, \"\n",
    "          f\"failed: {stats['failed']}, timeout: {stats['timeout']}\")\n",
    "\n",
    "# Final summary\n",
    "\n",
    "print(f\"Ingestion complete.\")\n",
    "print(f\"  Success:            {stats['success']}\")\n",
    "print(f\"  Failed:             {stats['failed']}\")\n",
    "print(f\"  Timeout:            {stats['timeout']}\")\n",
    "print(f\"  Skipped (unchanged):{stats['skipped_unchanged']}\")\n",
    "print(f\"  Skipped (circuit):  {stats['skipped_circuit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71962e12",
   "metadata": {},
   "source": [
    "## Verify Ingestion Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4507451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in CANDIDATE_SSP_DOCUMENT_CONTENT: 20\n",
      "\n",
      "Fetch status breakdown:\n",
      "FETCH_STATUS  CNT  AVG_SIZE  MAX_SIZE\n",
      "     success   20  574138.0    659670\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>CONTENT_HASH</th>\n",
       "      <th>CONTENT_SIZE_BYTES</th>\n",
       "      <th>HTTP_STATUS</th>\n",
       "      <th>FETCH_STATUS</th>\n",
       "      <th>FIRST_FETCHED_AT</th>\n",
       "      <th>LAST_FETCHED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/appendices</td>\n",
       "      <td>bfa5a1a112e0323513ce3c2f41fac00986baeae88e30a2...</td>\n",
       "      <td>524385</td>\n",
       "      <td>200</td>\n",
       "      <td>success</td>\n",
       "      <td>2026-02-07 08:55:04</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>0d9f12696d03cdb280fb751324798a0531e230cfc61977...</td>\n",
       "      <td>546182</td>\n",
       "      <td>200</td>\n",
       "      <td>success</td>\n",
       "      <td>2026-02-07 08:55:04</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>a1012897adffdf90854b6299992275d4b8215eaad9a90e...</td>\n",
       "      <td>575273</td>\n",
       "      <td>200</td>\n",
       "      <td>success</td>\n",
       "      <td>2026-02-07 08:55:04</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>f452ecca7f6fe12a8293d4ea051abae062c49c974b2ad2...</td>\n",
       "      <td>557892</td>\n",
       "      <td>200</td>\n",
       "      <td>success</td>\n",
       "      <td>2026-02-07 08:55:04</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.snowflake.com/en/api-reference</td>\n",
       "      <td>43a3e7509e6aff9981eb704412010e0f3be84a3e61868d...</td>\n",
       "      <td>659670</td>\n",
       "      <td>200</td>\n",
       "      <td>success</td>\n",
       "      <td>2026-02-07 08:55:04</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 LOC  \\\n",
       "0           https://docs.snowflake.com/en/appendices   \n",
       "1  https://docs.snowflake.com/en/collaboration/co...   \n",
       "2  https://docs.snowflake.com/en/collaboration/co...   \n",
       "3  https://docs.snowflake.com/en/collaboration/co...   \n",
       "4        https://docs.snowflake.com/en/api-reference   \n",
       "\n",
       "                                        CONTENT_HASH  CONTENT_SIZE_BYTES  \\\n",
       "0  bfa5a1a112e0323513ce3c2f41fac00986baeae88e30a2...              524385   \n",
       "1  0d9f12696d03cdb280fb751324798a0531e230cfc61977...              546182   \n",
       "2  a1012897adffdf90854b6299992275d4b8215eaad9a90e...              575273   \n",
       "3  f452ecca7f6fe12a8293d4ea051abae062c49c974b2ad2...              557892   \n",
       "4  43a3e7509e6aff9981eb704412010e0f3be84a3e61868d...              659670   \n",
       "\n",
       "   HTTP_STATUS FETCH_STATUS     FIRST_FETCHED_AT      LAST_FETCHED_AT  \n",
       "0          200      success  2026-02-07 08:55:04  2026-02-09 05:30:59  \n",
       "1          200      success  2026-02-07 08:55:04  2026-02-09 05:30:59  \n",
       "2          200      success  2026-02-07 08:55:04  2026-02-09 05:30:59  \n",
       "3          200      success  2026-02-07 08:55:04  2026-02-09 05:30:59  \n",
       "4          200      success  2026-02-07 08:55:04  2026-02-09 05:30:59  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row count\n",
    "content_count = cur.execute(f\"SELECT COUNT(*) FROM {CONTENT_TABLE}\").fetchone()[0]\n",
    "print(f\"Rows in {CONTENT_TABLE}: {content_count}\")\n",
    "\n",
    "# Fetch status breakdown\n",
    "df_status = pd.read_sql(f\"\"\"\n",
    "    SELECT FETCH_STATUS,\n",
    "           COUNT(*)            AS CNT,\n",
    "           ROUND(AVG(CONTENT_SIZE_BYTES))  AS AVG_SIZE,\n",
    "           MAX(CONTENT_SIZE_BYTES)         AS MAX_SIZE\n",
    "    FROM {CONTENT_TABLE}\n",
    "    GROUP BY FETCH_STATUS\n",
    "\"\"\", conn)\n",
    "print(\"\\nFetch status breakdown:\")\n",
    "print(df_status.to_string(index=False))\n",
    "\n",
    "# Sample successful rows\n",
    "df_content_sample = pd.read_sql(f\"\"\"\n",
    "    SELECT LOC, CONTENT_HASH, CONTENT_SIZE_BYTES, HTTP_STATUS,\n",
    "           FETCH_STATUS, FIRST_FETCHED_AT, LAST_FETCHED_AT\n",
    "    FROM {CONTENT_TABLE}\n",
    "    WHERE FETCH_STATUS = 'success'\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "df_content_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5b4a9",
   "metadata": {},
   "source": [
    "## Content Change Detection Test\n",
    "\n",
    "Re-run the ingestion for the same URLs. Since content hasn't changed (same `LASTMOD`), they should all be **skipped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1d79cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-run check on 20 URLs:\n",
      "  Skipped (already have content): 20\n",
      "  Would need fetch:               0\n",
      "  Rows in table before:           20\n",
      "  Change detection working — 20/20 URLs skipped on re-run\n"
     ]
    }
   ],
   "source": [
    "# Re-run ingestion on same URLs — should skip all (content unchanged)\n",
    "rerun_urls = master_urls.head(FETCH_LIMIT if FETCH_LIMIT else 20)\n",
    "rerun_locs = rerun_urls[\"LOC\"].tolist()\n",
    "\n",
    "existing_rerun = get_existing_content_info(cur, rerun_locs)\n",
    "\n",
    "rerun_skipped = 0\n",
    "rerun_needs_fetch = 0\n",
    "for loc in rerun_locs:\n",
    "    if loc in existing_rerun:\n",
    "        prev_hash, consec_fails = existing_rerun[loc]\n",
    "        if prev_hash is not None:\n",
    "            rerun_skipped += 1\n",
    "            continue\n",
    "    rerun_needs_fetch += 1\n",
    "\n",
    "count_before = cur.execute(f\"SELECT COUNT(*) FROM {CONTENT_TABLE}\").fetchone()[0]\n",
    "\n",
    "print(f\"Re-run check on {len(rerun_locs)} URLs:\")\n",
    "print(f\"  Skipped (already have content): {rerun_skipped}\")\n",
    "print(f\"  Would need fetch:               {rerun_needs_fetch}\")\n",
    "print(f\"  Rows in table before:           {count_before}\")\n",
    "print(f\"  Change detection working — {rerun_skipped}/{len(rerun_locs)} URLs skipped on re-run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9331587",
   "metadata": {},
   "source": [
    "# Task 4: Baseline Analytics Queries\n",
    "\n",
    "SQL analytics against the pipeline tables. All queries use explicit `ORDER BY` for deterministic output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507dd14",
   "metadata": {},
   "source": [
    "## 4a - Document Count Grouped by Source Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1abc3fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4a — Document count by source identifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_IDENTIFIER</th>\n",
       "      <th>DOCUMENT_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>6617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://other-docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 SOURCE_IDENTIFIER  DOCUMENT_COUNT\n",
       "0        https://docs.snowflake.com/en/sitemap.xml            6617\n",
       "1  https://other-docs.snowflake.com/en/sitemap.xml               3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4a = f\"\"\"\n",
    "SELECT SOURCE_SITEMAP              AS SOURCE_IDENTIFIER,\n",
    "       COUNT(DISTINCT LOC)         AS DOCUMENT_COUNT\n",
    "FROM   {STAGING_TABLE}\n",
    "GROUP  BY SOURCE_SITEMAP\n",
    "ORDER  BY DOCUMENT_COUNT DESC, SOURCE_IDENTIFIER ASC\n",
    "\"\"\"\n",
    "\n",
    "df_4a = pd.read_sql(query_4a, conn)\n",
    "print(\"4a — Document count by source identifier\")\n",
    "df_4a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17db62a",
   "metadata": {},
   "source": [
    "## 4b - Monthly Document Distribution (Trailing 12-Month Window)\n",
    "\n",
    "Documents binned by their `LASTMOD` month (upstream sitemap metadata, **not** a pipeline timestamp). Only months within the trailing 12-month window are included. `LASTMOD` is NULL for the entire Snowflake docs corpus since the sitemap omits `<lastmod>` tags — these are reported separately to surface the data gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f01547be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4b — Monthly document distribution (trailing 12 months)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LASTMOD_MONTH</th>\n",
       "      <th>DOCUMENT_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL (no lastmod)</td>\n",
       "      <td>6620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LASTMOD_MONTH  DOCUMENT_COUNT\n",
       "0  NULL (no lastmod)            6620"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4b = f\"\"\"\n",
    "WITH monthly AS (\n",
    "    SELECT STRFTIME('%Y-%m', LASTMOD)  AS LASTMOD_MONTH, COUNT(*) AS DOCUMENT_COUNT\n",
    "    FROM   {MASTER_TABLE}\n",
    "    WHERE  LASTMOD IS NOT NULL\n",
    "    AND    DATE(LASTMOD) >= DATE('now', '-12 months')\n",
    "    GROUP  BY LASTMOD_MONTH\n",
    "),\n",
    "null_counts AS (\n",
    "    SELECT 'NULL (no lastmod)' AS LASTMOD_MONTH, COUNT(*) AS DOCUMENT_COUNT\n",
    "    FROM   {MASTER_TABLE}\n",
    "    WHERE  LASTMOD IS NULL\n",
    ")\n",
    "SELECT * FROM monthly\n",
    "UNION ALL\n",
    "SELECT * FROM null_counts\n",
    "ORDER  BY LASTMOD_MONTH ASC\n",
    "\"\"\"\n",
    "\n",
    "df_4b = pd.read_sql(query_4b, conn)\n",
    "print(\"4b — Monthly document distribution (trailing 12 months)\")\n",
    "df_4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefc6a4",
   "metadata": {},
   "source": [
    "## 4c - Content Fetch Success Rate, Segmented by Source\n",
    "\n",
    "Joins `DOCUMENT_CONTENT` with `SITEMAP_STAGING` to attribute fetch outcomes back to each source sitemap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d159b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c — Content fetch success rate by source\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_SITEMAP</th>\n",
       "      <th>TOTAL_FETCHED</th>\n",
       "      <th>SUCCESS_COUNT</th>\n",
       "      <th>FAILED_COUNT</th>\n",
       "      <th>TIMEOUT_COUNT</th>\n",
       "      <th>SUCCESS_RATE_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              SOURCE_SITEMAP  TOTAL_FETCHED  SUCCESS_COUNT  \\\n",
       "0  https://docs.snowflake.com/en/sitemap.xml             20             20   \n",
       "\n",
       "   FAILED_COUNT  TIMEOUT_COUNT  SUCCESS_RATE_PCT  \n",
       "0             0              0             100.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4c = f\"\"\"\n",
    "SELECT s.SOURCE_SITEMAP,\n",
    "       COUNT(DISTINCT c.LOC)                                          AS TOTAL_FETCHED,\n",
    "       SUM(CASE WHEN c.FETCH_STATUS = 'success' THEN 1 ELSE 0 END)   AS SUCCESS_COUNT,\n",
    "       SUM(CASE WHEN c.FETCH_STATUS = 'failed'  THEN 1 ELSE 0 END)   AS FAILED_COUNT,\n",
    "       SUM(CASE WHEN c.FETCH_STATUS = 'timeout' THEN 1 ELSE 0 END)   AS TIMEOUT_COUNT,\n",
    "       ROUND(\n",
    "         SUM(CASE WHEN c.FETCH_STATUS = 'success' THEN 1 ELSE 0 END) * 100.0\n",
    "         / MAX(COUNT(DISTINCT c.LOC), 1),\n",
    "         2\n",
    "       )                                                              AS SUCCESS_RATE_PCT\n",
    "FROM   {CONTENT_TABLE} c\n",
    "JOIN   {STAGING_TABLE}  s ON c.LOC = s.LOC\n",
    "GROUP  BY s.SOURCE_SITEMAP\n",
    "ORDER  BY s.SOURCE_SITEMAP ASC\n",
    "\"\"\"\n",
    "\n",
    "df_4c = pd.read_sql(query_4c, conn)\n",
    "print(\"4c — Content fetch success rate by source\")\n",
    "df_4c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c3e71",
   "metadata": {},
   "source": [
    "## 4d - Top 10 URL Path Segments by Frequency\n",
    "\n",
    "Extracts the **first meaningful path component** after stripping the language prefix (`/en/`, `/de/`, `/fr/`, etc.). This reveals the actual documentation sections (e.g. `sql-reference`, `developer-guide`, `collaboration`) rather than the locale routing prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2bc9179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d — Top 10 URL path segments (language prefix stripped)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRST_PATH_SEGMENT</th>\n",
       "      <th>FREQUENCY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sql-reference</td>\n",
       "      <td>2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user-guide</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>release-notes</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>developer-guide</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>migrations</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>connectors</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>collaboration</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>progaccess</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>index</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>search</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FIRST_PATH_SEGMENT  FREQUENCY\n",
       "0      sql-reference       2152\n",
       "1         user-guide       1605\n",
       "2      release-notes       1351\n",
       "3    developer-guide        847\n",
       "4         migrations        459\n",
       "5         connectors        108\n",
       "6      collaboration         60\n",
       "7         progaccess          3\n",
       "8              index          2\n",
       "9             search          2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4d = f\"\"\"\n",
    "-- 4d: Top 10 URL path segments — documentation sections\n",
    "-- Language prefixes (/en/, /de/, /fr/, /ja/, /ko/, /pt/) are stripped so the\n",
    "-- analysis describes documentation structure, not locale routing.\n",
    "WITH url_paths AS (\n",
    "    SELECT\n",
    "        SUBSTR(LOC,\n",
    "               INSTR(LOC, '//') + 2\n",
    "               + INSTR(SUBSTR(LOC, INSTR(LOC, '//') + 2), '/')\n",
    "        ) AS raw_path\n",
    "    FROM {MASTER_TABLE}\n",
    "),\n",
    "cleaned AS (\n",
    "    SELECT\n",
    "        CASE\n",
    "            -- Strip 2-letter language prefix (en/, de/, fr/, ja/, ko/, pt/)\n",
    "            WHEN raw_path LIKE '__/%'\n",
    "                 AND SUBSTR(raw_path, 1, 2) IN ('en','de','fr','ja','ko','pt')\n",
    "            THEN SUBSTR(raw_path, 4)            -- skip 'xx/'\n",
    "            ELSE raw_path\n",
    "        END AS url_path\n",
    "    FROM url_paths\n",
    "    WHERE raw_path IS NOT NULL AND raw_path != ''\n",
    "),\n",
    "segments AS (\n",
    "    SELECT\n",
    "        CASE\n",
    "            WHEN INSTR(url_path, '/') > 0\n",
    "                THEN SUBSTR(url_path, 1, INSTR(url_path, '/') - 1)\n",
    "            ELSE url_path\n",
    "        END AS FIRST_PATH_SEGMENT\n",
    "    FROM cleaned\n",
    "    WHERE url_path IS NOT NULL AND url_path != ''\n",
    ")\n",
    "SELECT FIRST_PATH_SEGMENT,\n",
    "       COUNT(*) AS FREQUENCY\n",
    "FROM   segments\n",
    "GROUP  BY FIRST_PATH_SEGMENT\n",
    "ORDER  BY FREQUENCY DESC, FIRST_PATH_SEGMENT ASC\n",
    "LIMIT  10\n",
    "\"\"\"\n",
    "\n",
    "df_4d = pd.read_sql(query_4d, conn)\n",
    "print(\"4d — Top 10 URL path segments (language prefix stripped)\")\n",
    "df_4d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbbe845",
   "metadata": {},
   "source": [
    "## 4e - Stale Document Analysis\n",
    "\n",
    "The Snowflake documentation sitemap does not provide `<lastmod>` timestamps for individual URLs. `LASTMOD` is therefore preserved as nullable upstream metadata — it is **not** replaced with crawl time or inferred from page content.\n",
    "\n",
    "\n",
    "Temporal analysis relies on the pipeline observation fields:A URL whose `LAST_SEEN_AT` lags the current date by 180+ days has **disappeared from all sitemaps** and is flagged as potentially removed.\n",
    "\n",
    "- `FIRST_SEEN_AT` — when the pipeline first discovered the URL\n",
    "- `LAST_SEEN_AT` — the most recent pipeline run in which the URL was still present in a sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "813f12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4e — Stale document analysis (pipeline observation-based: LAST_SEEN_AT tracks URL presence across runs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTAL_DOCUMENTS</th>\n",
       "      <th>NULL_LASTMOD_COUNT</th>\n",
       "      <th>NULL_LASTMOD_PCT</th>\n",
       "      <th>STALE_BY_LASTMOD</th>\n",
       "      <th>STALE_BY_LAST_SEEN</th>\n",
       "      <th>STALE_LAST_SEEN_PCT</th>\n",
       "      <th>FRESH_BY_LAST_SEEN</th>\n",
       "      <th>FRESH_LAST_SEEN_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6620</td>\n",
       "      <td>6620</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6620</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOTAL_DOCUMENTS  NULL_LASTMOD_COUNT  NULL_LASTMOD_PCT  STALE_BY_LASTMOD  \\\n",
       "0             6620                6620             100.0                 0   \n",
       "\n",
       "   STALE_BY_LAST_SEEN  STALE_LAST_SEEN_PCT  FRESH_BY_LAST_SEEN  \\\n",
       "0                   0                  0.0                6620   \n",
       "\n",
       "   FRESH_LAST_SEEN_PCT  \n",
       "0                100.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4e = f\"\"\"\n",
    "-- 4e: Stale document analysis\n",
    "-- LASTMOD = optional upstream sitemap metadata; NULL for the entire corpus\n",
    "-- (Snowflake's sitemaps do not provide <lastmod> tags).  Not fabricated.\n",
    "-- FIRST_SEEN_AT / LAST_SEEN_AT = pipeline observation timestamps.\n",
    "-- LAST_SEEN_AT tracks when the URL was last observed in any sitemap.\n",
    "-- URLs whose LAST_SEEN_AT lags by 180+ days have disappeared → flagged stale.\n",
    "WITH doc_stats AS (\n",
    "    SELECT\n",
    "        COUNT(*)                                                             AS TOTAL_DOCUMENTS,\n",
    "        SUM(CASE WHEN LASTMOD IS NOT NULL\n",
    "                  AND DATE(LASTMOD) < DATE('now', '-180 days')\n",
    "             THEN 1 ELSE 0 END)                                             AS STALE_BY_LASTMOD,\n",
    "        SUM(CASE WHEN LASTMOD IS NULL THEN 1 ELSE 0 END)                    AS NULL_LASTMOD_COUNT,\n",
    "        SUM(CASE WHEN LAST_SEEN_AT IS NOT NULL\n",
    "                  AND DATE(LAST_SEEN_AT) < DATE('now', '-180 days')\n",
    "             THEN 1 ELSE 0 END)                                             AS STALE_BY_LAST_SEEN,\n",
    "        SUM(CASE WHEN LAST_SEEN_AT IS NOT NULL\n",
    "                  AND DATE(LAST_SEEN_AT) >= DATE('now', '-180 days')\n",
    "             THEN 1 ELSE 0 END)                                             AS FRESH_BY_LAST_SEEN\n",
    "    FROM {MASTER_TABLE}\n",
    ")\n",
    "SELECT TOTAL_DOCUMENTS,\n",
    "       NULL_LASTMOD_COUNT,\n",
    "       ROUND(NULL_LASTMOD_COUNT * 100.0 / MAX(TOTAL_DOCUMENTS, 1), 2)       AS NULL_LASTMOD_PCT,\n",
    "       STALE_BY_LASTMOD,\n",
    "       STALE_BY_LAST_SEEN,\n",
    "       ROUND(STALE_BY_LAST_SEEN * 100.0 / MAX(TOTAL_DOCUMENTS, 1), 2)      AS STALE_LAST_SEEN_PCT,\n",
    "       FRESH_BY_LAST_SEEN,\n",
    "       ROUND(FRESH_BY_LAST_SEEN * 100.0 / MAX(TOTAL_DOCUMENTS, 1), 2)      AS FRESH_LAST_SEEN_PCT\n",
    "FROM   doc_stats\n",
    "\"\"\"\n",
    "\n",
    "df_4e = pd.read_sql(query_4e, conn)\n",
    "print(\"4e — Stale document analysis (pipeline observation-based: LAST_SEEN_AT tracks URL presence across runs)\")\n",
    "df_4e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b3c87",
   "metadata": {},
   "source": [
    "# Task 5: Query Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc212976",
   "metadata": {},
   "source": [
    "## Scenario 1 - Documents Modified Within a 7-Day Rolling Window\n",
    "\n",
    "**Optimization Axes: Cost - Latency**\n",
    "\n",
    "- **Cost** in a data warehouse = compute credits × time × data scanned.\n",
    "- **Latency** = wall-clock time for the query to return results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03d3636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST-EFFICIENT APPROACH  (full scan, no index)\n",
      "Rows returned: 6620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>EFFECTIVE_DATE</th>\n",
       "      <th>SOURCES</th>\n",
       "      <th>LAST_SEEN_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.snowflake.com/en/api-reference</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://docs.snowflake.com/en/appendices</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://docs.snowflake.com/en/collaboration/co...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 LOC       EFFECTIVE_DATE  \\\n",
       "0        https://docs.snowflake.com/en/api-reference  2026-02-09 05:30:59   \n",
       "1           https://docs.snowflake.com/en/appendices  2026-02-09 05:30:59   \n",
       "2  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "3  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "4  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "5  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "6  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "7  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "8  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "9  https://docs.snowflake.com/en/collaboration/co...  2026-02-09 05:30:59   \n",
       "\n",
       "                                     SOURCES         LAST_SEEN_AT  \n",
       "0  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "1  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "2  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "3  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "4  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "5  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "6  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "7  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "8  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "9  https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Cost-efficient approach\n",
    "\n",
    "Rationale:\n",
    "This query uses a simple sequential scan with a function-wrapped filter\n",
    "(DATE(COALESCE(...))). No indexes are created or maintained, so there is\n",
    "no write-amplification overhead. INSERT, UPDATE, and DELETE operations\n",
    "only touch the base table.\n",
    "\n",
    "In a Snowflake warehouse, an XS warehouse is usually sufficient because\n",
    "the scan is single-threaded. There are no clustering keys, so there is no\n",
    "automatic clustering credit usage, and storage cost is limited to the\n",
    "base table without any index overhead.\n",
    "\n",
    "Tradeoff:\n",
    "Each execution performs a full table scan. For occasional or ad-hoc\n",
    "queries this is acceptable because you only pay at query time. However,\n",
    "if the query runs very frequently (for example a dashboard refreshing\n",
    "every minute), the accumulated scan cost can become noticeable.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_s1_cost = f\"\"\"\n",
    "-- Scenario 1: 7-day rolling window — COST-EFFICIENT\n",
    "-- LASTMOD = optional upstream sitemap metadata (NULL for this corpus).\n",
    "-- LAST_SEEN_AT = pipeline observation timestamp (when URL was last observed).\n",
    "-- COALESCE gives the best available temporal signal for the rolling-window filter.\n",
    "SELECT LOC,\n",
    "       COALESCE(LASTMOD, LAST_SEEN_AT)                AS EFFECTIVE_DATE,\n",
    "       SOURCES,\n",
    "       LAST_SEEN_AT\n",
    "FROM   {MASTER_TABLE}\n",
    "WHERE  DATE(COALESCE(LASTMOD, LAST_SEEN_AT)) >= DATE('now', '-7 days')\n",
    "ORDER  BY EFFECTIVE_DATE DESC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"COST-EFFICIENT APPROACH  (full scan, no index)\")\n",
    "\n",
    "df_s1_cost = pd.read_sql(query_s1_cost, conn)\n",
    "\n",
    "print(f\"Rows returned: {len(df_s1_cost)}\") \n",
    "df_s1_cost.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a8b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CANDIDATE_SSP_DOCS_MASTER_OPTIMIZED with EFFECTIVE_DATE column + index\n",
      "================================================================================\n",
      "TIME-EFFICIENT APPROACH  (indexed precomputed column)\n",
      "================================================================================\n",
      "Rows returned: 6620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>EFFECTIVE_DATE</th>\n",
       "      <th>SOURCES</th>\n",
       "      <th>LAST_SEEN_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://other-docs.snowflake.com/en/search</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://other-docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://other-docs.snowflake.com/en/index</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://other-docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://other-docs.snowflake.com/en/connectors</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://other-docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.snowflake.com/en/search</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.snowflake.com/en/release-notes/al...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://docs.snowflake.com/en/user-guide/views...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://docs.snowflake.com/en/user-guide/views...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://docs.snowflake.com/en/user-guide/views...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://docs.snowflake.com/en/user-guide/views...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://docs.snowflake.com/en/user-guide/views...</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "      <td>https://docs.snowflake.com/en/sitemap.xml</td>\n",
       "      <td>2026-02-09 05:30:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 LOC       EFFECTIVE_DATE  \\\n",
       "0         https://other-docs.snowflake.com/en/search  2026-02-09 05:30:59   \n",
       "1          https://other-docs.snowflake.com/en/index  2026-02-09 05:30:59   \n",
       "2     https://other-docs.snowflake.com/en/connectors  2026-02-09 05:30:59   \n",
       "3               https://docs.snowflake.com/en/search  2026-02-09 05:30:59   \n",
       "4  https://docs.snowflake.com/en/release-notes/al...  2026-02-09 05:30:59   \n",
       "5  https://docs.snowflake.com/en/user-guide/views...  2026-02-09 05:30:59   \n",
       "6  https://docs.snowflake.com/en/user-guide/views...  2026-02-09 05:30:59   \n",
       "7  https://docs.snowflake.com/en/user-guide/views...  2026-02-09 05:30:59   \n",
       "8  https://docs.snowflake.com/en/user-guide/views...  2026-02-09 05:30:59   \n",
       "9  https://docs.snowflake.com/en/user-guide/views...  2026-02-09 05:30:59   \n",
       "\n",
       "                                           SOURCES         LAST_SEEN_AT  \n",
       "0  https://other-docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "1  https://other-docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "2  https://other-docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "3        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "4        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "5        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "6        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "7        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "8        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  \n",
       "9        https://docs.snowflake.com/en/sitemap.xml  2026-02-09 05:30:59  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Time-efficient (low-latency) approach\n",
    "\n",
    "Rationale:\n",
    "This approach precomputes a materialized column and supports it with an\n",
    "index. The planner can perform an index range scan (O(log n + k)) instead\n",
    "of a full table scan (O(n)), so query latency stays low even as the table\n",
    "grows to millions of rows.\n",
    "\n",
    "EFFECTIVE_DATE = COALESCE(LASTMOD, LAST_SEEN_AT)\n",
    "\n",
    "LASTMOD is optional upstream sitemap metadata (often NULL in this dataset).\n",
    "LAST_SEEN_AT is the pipeline observation timestamp. Using COALESCE provides\n",
    "the best available temporal signal for filtering.\n",
    "\n",
    "Key design ideas:\n",
    "A stored generated column (EFFECTIVE_DATE) avoids wrapping the filter\n",
    "column in a function. When a filter uses a function like DATE(COALESCE(...)),\n",
    "the optimizer cannot efficiently use a normal index. Indexing the generated\n",
    "column directly allows the predicate `WHERE EFFECTIVE_DATE >= ...` to seek\n",
    "into the index. Matching the ORDER BY with the index also removes the need\n",
    "for a separate sort step.\n",
    "\n",
    "Tradeoff:\n",
    "This approach increases cost. The index requires additional storage and\n",
    "every INSERT or UPDATE must maintain it, which adds write overhead. In\n",
    "Snowflake this corresponds to defining a CLUSTER BY key. The system\n",
    "reorganizes micro-partitions in the background and consumes credits, but\n",
    "date-filtered queries benefit from partition pruning and return very quickly\n",
    "even on large tables.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Step 1: Create optimized table with precomputed EFFECTIVE_DATE + index (one-time DDL cost)\n",
    "cur.execute(f\"DROP TABLE IF EXISTS {MASTER_TABLE}_OPTIMIZED\")\n",
    "cur.execute(f\"\"\"\n",
    "    CREATE TABLE {MASTER_TABLE}_OPTIMIZED AS\n",
    "    SELECT *,\n",
    "           COALESCE(LASTMOD, LAST_SEEN_AT) AS EFFECTIVE_DATE\n",
    "    FROM   {MASTER_TABLE}\n",
    "\"\"\")\n",
    "cur.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_effective_date\n",
    "    ON {MASTER_TABLE}_OPTIMIZED (EFFECTIVE_DATE)\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(f\"Created {MASTER_TABLE}_OPTIMIZED with EFFECTIVE_DATE column + index\")\n",
    "\n",
    "# Step 2: Query using index range scan on precomputed EFFECTIVE_DATE\n",
    "query_s1_time = f\"\"\"\n",
    "-- Scenario 1: 7-day rolling window — TIME-EFFICIENT\n",
    "-- Index range scan on precomputed EFFECTIVE_DATE; avoids DATE() wrapper\n",
    "SELECT LOC,\n",
    "       EFFECTIVE_DATE,\n",
    "       SOURCES,\n",
    "       LAST_SEEN_AT\n",
    "FROM   {MASTER_TABLE}_OPTIMIZED\n",
    "WHERE  EFFECTIVE_DATE >= STRFTIME('%Y-%m-%dT%H:%M:%S', 'now', '-7 days')\n",
    "ORDER  BY EFFECTIVE_DATE DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIME-EFFICIENT APPROACH  (indexed precomputed column)\")\n",
    "print(\"=\" * 80)\n",
    "df_s1_time = pd.read_sql(query_s1_time, conn)\n",
    "print(f\"Rows returned: {len(df_s1_time)}\")\n",
    "df_s1_time.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4eb8d",
   "metadata": {},
   "source": [
    "## Scenario 2 — Unique URL Count per Source with Mean Content Length\n",
    "\n",
    "**Optimization Axes: Compute - Parallelism**\n",
    "\n",
    "- **Compute-efficient** = minimise total CPU / memory by doing everything in one pass.\n",
    "- **Parallelism-efficient** = structure the query so the engine can fan out independent work items across multiple threads / nodes, reducing wall-clock time at the expense of more total compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e76200b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPUTE-EFFICIENT APPROACH  (single pass join + group)\n",
      "================================================================================\n",
      "                           SOURCE_SITEMAP  UNIQUE_URL_COUNT  MEAN_CONTENT_LENGTH_BYTES\n",
      "https://docs.snowflake.com/en/sitemap.xml                20                   574138.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute-efficient approach\n",
    "\n",
    "Rationale:\n",
    "This query computes both metrics in a single JOIN followed by a single\n",
    "GROUP BY. The planner builds one aggregation structure and calculates\n",
    "COUNT(DISTINCT) and AVG() together, rather than running multiple passes\n",
    "over the data.\n",
    "\n",
    "Why this reduces compute:\n",
    "Each base table is scanned only once. The plan performs one join and one\n",
    "aggregation step, which avoids unnecessary intermediate materialization.\n",
    "The memory usage grows only with the number of distinct sources being\n",
    "grouped.\n",
    "\n",
    "Tradeoff:\n",
    "The plan forms a single processing chain (scan → join → aggregate → sort).\n",
    "Because both metrics depend on the same grouped result set, the engine\n",
    "cannot parallelize them independently. This limits how much the warehouse\n",
    "can distribute the work across multiple workers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_s2_compute = f\"\"\"\n",
    "-- Scenario 2: URL count + mean content length — COMPUTE-EFFICIENT\n",
    "-- Single join, single GROUP BY: minimises total work\n",
    "SELECT s.SOURCE_SITEMAP,\n",
    "       COUNT(DISTINCT s.LOC)                     AS UNIQUE_URL_COUNT,\n",
    "       ROUND(AVG(c.CONTENT_SIZE_BYTES), 2)       AS MEAN_CONTENT_LENGTH_BYTES\n",
    "FROM   {STAGING_TABLE}  s\n",
    "JOIN   {CONTENT_TABLE}  c  ON s.LOC = c.LOC\n",
    "GROUP  BY s.SOURCE_SITEMAP\n",
    "ORDER  BY UNIQUE_URL_COUNT DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPUTE-EFFICIENT APPROACH  (single pass join + group)\")\n",
    "print(\"=\" * 80)\n",
    "df_s2_compute = pd.read_sql(query_s2_compute, conn)\n",
    "print(df_s2_compute.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64fb3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARALLELISM-EFFICIENT APPROACH \n",
      "                                 SOURCE_SITEMAP  UNIQUE_URL_COUNT  MEAN_CONTENT_LENGTH_BYTES\n",
      "      https://docs.snowflake.com/en/sitemap.xml              6617                   574138.1\n",
      "https://other-docs.snowflake.com/en/sitemap.xml                 3                        0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parallelism-efficient (low-latency) approach\n",
    "\n",
    "Rationale:\n",
    "The query is split into two independent CTEs — one calculating the distinct\n",
    "URL count and the other computing the average content length. Each CTE works\n",
    "on a different base table and performs its own aggregation, so the query\n",
    "engine can execute them at the same time on separate workers.\n",
    "\n",
    "Why this improves parallelism:\n",
    "The `url_counts` CTE scans only the SITEMAP_STAGING table, while the\n",
    "`avg_sizes` CTE scans DOCUMENT_CONTENT and joins it with the staging data.\n",
    "Since the two CTEs do not depend on each other, they can run concurrently.\n",
    "The final join combines two small aggregated result sets (one row per source),\n",
    "so it adds almost no overhead.\n",
    "\n",
    "In Snowflake, a multi-cluster warehouse can schedule the two scans on\n",
    "different processing nodes within the same query, which reduces wall-clock\n",
    "runtime compared to a fully serial plan.\n",
    "\n",
    "Tradeoff:\n",
    "Overall compute usage increases because the staging data is scanned twice and\n",
    "two aggregation structures must be maintained in memory. The query uses more\n",
    "CPU and I/O in total, but parallel execution reduces the elapsed time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_s2_parallel = f\"\"\"\n",
    "-- Scenario 2: URL count + mean content length — PARALLELISM-EFFICIENT\n",
    "-- Two independent CTEs → engine can parallelise them across workers\n",
    "WITH url_counts AS (\n",
    "    -- CTE 1: count distinct URLs per source (STAGING only — no join)\n",
    "    SELECT SOURCE_SITEMAP,\n",
    "           COUNT(DISTINCT LOC)                   AS UNIQUE_URL_COUNT\n",
    "    FROM   {STAGING_TABLE}\n",
    "    GROUP  BY SOURCE_SITEMAP\n",
    "),\n",
    "avg_sizes AS (\n",
    "    -- CTE 2: mean content size per source (requires join, runs in parallel)\n",
    "    SELECT s.SOURCE_SITEMAP,\n",
    "           ROUND(AVG(c.CONTENT_SIZE_BYTES), 2)   AS MEAN_CONTENT_LENGTH_BYTES\n",
    "    FROM   {CONTENT_TABLE}  c\n",
    "    JOIN   {STAGING_TABLE}   s  ON c.LOC = s.LOC\n",
    "    GROUP  BY s.SOURCE_SITEMAP\n",
    ")\n",
    "SELECT u.SOURCE_SITEMAP,\n",
    "       u.UNIQUE_URL_COUNT,\n",
    "       COALESCE(a.MEAN_CONTENT_LENGTH_BYTES, 0)  AS MEAN_CONTENT_LENGTH_BYTES\n",
    "FROM   url_counts u\n",
    "LEFT   JOIN avg_sizes a ON u.SOURCE_SITEMAP = a.SOURCE_SITEMAP\n",
    "ORDER  BY u.UNIQUE_URL_COUNT DESC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"PARALLELISM-EFFICIENT APPROACH \")\n",
    "\n",
    "df_s2_parallel = pd.read_sql(query_s2_parallel, conn)\n",
    "print(df_s2_parallel.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e720558",
   "metadata": {},
   "source": [
    "## Scenario 3 - Content Deduplication Detection (Identical Hashes, Distinct URLs)\n",
    "\n",
    "**Optimization Axes: Join Complexity - Speed**\n",
    "\n",
    "- **Low join complexity** = avoid expensive self-joins; use aggregation to find duplicates.\n",
    "- **Speed** = minimise wall-clock time even if the join plan is more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c29620aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOW JOIN-COMPLEXITY APPROACH  (GROUP BY + HAVING)\n",
      "Duplicate hash groups found: 0\n",
      "(No duplicate content hashes detected — all 20 fetched docs are unique)\n",
      "Showing hash distribution instead:\n",
      " UNIQUE_HASHES  TOTAL_DOCS\n",
      "            20          20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Low join-complexity approach (aggregation only — no self-join)\n",
    "\n",
    "Rationale:\n",
    "Duplicates are detected by grouping on CONTENT_HASH and filtering with\n",
    "HAVING COUNT(*) > 1. The engine performs a single sequential scan,\n",
    "builds one aggregation structure keyed by CONTENT_HASH, and filters the\n",
    "duplicate groups during aggregation.\n",
    "\n",
    "Complexity:\n",
    "The work is essentially O(n) for the scan, with memory usage proportional\n",
    "to the number of distinct hashes. Because there is no self-join, the\n",
    "optimizer does not need to consider join order, join type, or which side\n",
    "to build and probe.\n",
    "\n",
    "Tradeoff:\n",
    "This approach returns summary rows (one per duplicate hash group) rather\n",
    "than the individual URLs inside each group. To retrieve the actual LOC\n",
    "values, a second query is required. It works well for reporting how many\n",
    "duplicates exist and which hashes are involved, but not for actions that\n",
    "require listing every affected URL.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_s3_simple = f\"\"\"\n",
    "-- Scenario 3: Content dedup — LOW JOIN-COMPLEXITY (aggregate only)\n",
    "-- Single scan, single GROUP BY, no self-join\n",
    "SELECT CONTENT_HASH,\n",
    "       COUNT(DISTINCT LOC)      AS DUPLICATE_URL_COUNT,\n",
    "       MIN(LOC)                 AS EXAMPLE_URL,\n",
    "       MAX(CONTENT_SIZE_BYTES)  AS CONTENT_SIZE\n",
    "FROM   {CONTENT_TABLE}\n",
    "WHERE  CONTENT_HASH IS NOT NULL\n",
    "GROUP  BY CONTENT_HASH\n",
    "HAVING COUNT(DISTINCT LOC) > 1\n",
    "ORDER  BY DUPLICATE_URL_COUNT DESC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"LOW JOIN-COMPLEXITY APPROACH  (GROUP BY + HAVING)\")\n",
    "\n",
    "df_s3_simple = pd.read_sql(query_s3_simple, conn)\n",
    "print(f\"Duplicate hash groups found: {len(df_s3_simple)}\")\n",
    "if len(df_s3_simple) > 0:\n",
    "    print(df_s3_simple.to_string(index=False))\n",
    "else:\n",
    "    print(\"(No duplicate content hashes detected — all 20 fetched docs are unique)\")\n",
    "    print(\"Showing hash distribution instead:\")\n",
    "    df_hash_dist = pd.read_sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT CONTENT_HASH) AS UNIQUE_HASHES,\n",
    "               COUNT(*)                     AS TOTAL_DOCS\n",
    "        FROM   {CONTENT_TABLE}\n",
    "        WHERE  CONTENT_HASH IS NOT NULL\n",
    "    \"\"\", conn)\n",
    "    print(df_hash_dist.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fdb3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEED-OPTIMISED APPROACH  (indexed semi-join, returns all duplicate URLs)\n",
      "Total duplicate URL rows: 0\n",
      "(No duplicate content detected across the 20 fetched documents)\n",
      "\n",
      "Query plan verification (EXPLAIN):\n",
      " id  parent  notused                                                                              detail\n",
      "  2       0        0                                                               CO-ROUTINE dup_hashes\n",
      "  9       2        0 SEARCH CANDIDATE_SSP_DOCUMENT_CONTENT USING INDEX idx_content_hash (CONTENT_HASH>?)\n",
      " 47       0        0                                                                              SCAN d\n",
      " 50       0        0                              SEARCH c USING INDEX idx_content_hash (CONTENT_HASH=?)\n",
      " 65       0        0                                                        USE TEMP B-TREE FOR ORDER BY\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Speed-optimized approach (self-join with index — returns full detail rows)\n",
    "\n",
    "Rationale:\n",
    "When the goal is to return every duplicate URL rather than just counts,\n",
    "the query must join back to the detailed rows. This uses an indexed\n",
    "semi-join pattern.\n",
    "\n",
    "First, an index is created on CONTENT_HASH so join lookups are logarithmic\n",
    "instead of scanning the whole table. A CTE identifies duplicate hashes\n",
    "using the same GROUP BY and HAVING logic, producing a small set of keys.\n",
    "The main query then joins this small set back to the base table, allowing\n",
    "the database to seek directly to matching rows.\n",
    "\n",
    "Complexity:\n",
    "The initial grouping costs O(n). The join then performs O(d × log n) work,\n",
    "where d is the number of duplicate URLs and is usually much smaller than n.\n",
    "\n",
    "Why this is faster for large tables:\n",
    "The duplicate hash set is small and can be distributed efficiently. The\n",
    "index prevents a second full table scan and the query returns complete\n",
    "detail rows in a single execution.\n",
    "\n",
    "Tradeoff:\n",
    "The plan is more complex and the index adds ongoing write overhead because\n",
    "inserts and updates must maintain it. The query is harder to debug, but\n",
    "on large datasets the index replaces a full rescan with targeted lookups,\n",
    "significantly reducing wall-clock time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Step 1: index on CONTENT_HASH (one-time cost)\n",
    "cur.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_content_hash\n",
    "    ON {CONTENT_TABLE} (CONTENT_HASH)\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "query_s3_fast = f\"\"\"\n",
    "-- Scenario 3: Content dedup — SPEED-OPTIMISED (indexed semi-join, full detail)\n",
    "-- Step A: identify duplicate hashes  (small result set)\n",
    "-- Step B: index-probe join to retrieve all URLs per duplicate hash\n",
    "WITH dup_hashes AS (\n",
    "    SELECT CONTENT_HASH\n",
    "    FROM   {CONTENT_TABLE}\n",
    "    WHERE  CONTENT_HASH IS NOT NULL\n",
    "    GROUP  BY CONTENT_HASH\n",
    "    HAVING COUNT(DISTINCT LOC) > 1\n",
    ")\n",
    "SELECT c.CONTENT_HASH,\n",
    "       c.LOC,\n",
    "       c.CONTENT_SIZE_BYTES,\n",
    "       c.FETCH_STATUS,\n",
    "       c.LAST_FETCHED_AT\n",
    "FROM   {CONTENT_TABLE}  c\n",
    "JOIN   dup_hashes        d  ON c.CONTENT_HASH = d.CONTENT_HASH\n",
    "ORDER  BY c.CONTENT_HASH, c.LOC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"SPEED-OPTIMISED APPROACH  (indexed semi-join, returns all duplicate URLs)\")\n",
    "\n",
    "df_s3_fast = pd.read_sql(query_s3_fast, conn)\n",
    "print(f\"Total duplicate URL rows: {len(df_s3_fast)}\")\n",
    "if len(df_s3_fast) > 0:\n",
    "    print(df_s3_fast.to_string(index=False))\n",
    "else:\n",
    "    print(\"(No duplicate content detected across the 20 fetched documents)\")\n",
    "    print(\"\\nQuery plan verification (EXPLAIN):\")\n",
    "    explain = pd.read_sql(f\"EXPLAIN QUERY PLAN {query_s3_fast}\", conn)\n",
    "    print(explain.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82116b",
   "metadata": {},
   "source": [
    "# Task 6: Testing\n",
    "\n",
    "**Objective:** Comprehensive test suite covering unit, integration, and data quality testing.\n",
    "\n",
    "**Framework:** pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f68adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/test_data_quality.py::TestSchemaValidation::test_staging_table_has_not_null_on_loc\n",
      "tests/test_data_quality.py::TestSchemaValidation::test_master_table_has_primary_key\n",
      "tests/test_data_quality.py::TestSchemaValidation::test_content_table_has_primary_key\n",
      "tests/test_data_quality.py::TestSchemaValidation::test_staging_table_columns\n",
      "tests/test_data_quality.py::TestSchemaValidation::test_master_table_columns\n",
      "tests/test_data_quality.py::TestSchemaValidation::test_content_table_columns\n",
      "tests/test_data_quality.py::TestSchemaValidation::test_content_table_default_values\n",
      "tests/test_data_quality.py::TestNullHandling::test_missing_lastmod_yields_none\n",
      "tests/test_data_quality.py::TestNullHandling::test_missing_loc_element_skipped\n",
      "tests/test_data_quality.py::TestNullHandling::test_compute_hash_empty_content\n",
      "tests/test_data_quality.py::TestNullHandling::test_fetch_document_failure_returns_none_content\n",
      "tests/test_data_quality.py::TestConstraintEnforcement::test_merge_uses_upsert_pattern\n",
      "tests/test_data_quality.py::TestConstraintEnforcement::test_merge_deduplicates_sources\n",
      "tests/test_data_quality.py::TestConstraintEnforcement::test_master_table_loc_is_varchar_2000\n",
      "tests/test_hashing_unit.py::TestComputeHash::test_deterministic\n",
      "tests/test_hashing_unit.py::TestComputeHash::test_different_content_different_hash\n",
      "tests/test_hashing_unit.py::TestComputeHash::test_matches_stdlib_sha256\n",
      "tests/test_hashing_unit.py::TestComputeHash::test_empty_string\n",
      "tests/test_hashing_unit.py::TestComputeHash::test_unicode_content\n",
      "tests/test_integration_consolidation.py::TestEndToEndConsolidation::test_full_pipeline_mock\n",
      "tests/test_integration_consolidation.py::TestEndToEndConsolidation::test_merge_sql_references_correct_tables\n",
      "tests/test_integration_consolidation.py::TestIdempotency::test_double_run_same_row_count\n",
      "tests/test_integration_consolidation.py::TestIdempotency::test_parse_sitemap_idempotent\n",
      "tests/test_integration_consolidation.py::TestMetricsTableDDL::test_create_metrics_table\n",
      "tests/test_integration_consolidation.py::TestAlertsTableDDL::test_create_alerts_table\n",
      "tests/test_integration_consolidation.py::TestAlertsTableDDL::test_alerts_severity_not_null\n",
      "tests/test_integration_consolidation.py::TestMetricsLifecycle::test_start_returns_running_status\n",
      "tests/test_integration_consolidation.py::TestMetricsLifecycle::test_finish_sets_completed\n",
      "tests/test_integration_consolidation.py::TestMetricsLifecycle::test_failure_rate_computed\n",
      "tests/test_integration_consolidation.py::TestMetricsLifecycle::test_failure_rate_zero_when_no_fetches\n",
      "tests/test_integration_consolidation.py::TestMetricsLifecycle::test_save_metrics_executes_insert\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_no_alert_on_healthy_run\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_warning_on_moderate_failure_rate\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_critical_on_high_failure_rate\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_boundary_exactly_at_warning\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_boundary_exactly_at_critical\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_empty_result_set_triggers_critical\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_no_empty_alert_when_rows_present\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_perf_degradation_alert\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_no_perf_alert_within_normal_range\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_no_perf_alert_when_no_history\n",
      "tests/test_integration_consolidation.py::TestEvaluateAlerts::test_multiple_alerts_at_once\n",
      "tests/test_integration_consolidation.py::TestStalenessAlert::test_no_runs_triggers_critical\n",
      "tests/test_integration_consolidation.py::TestStalenessAlert::test_recent_run_no_alert\n",
      "tests/test_integration_consolidation.py::TestStalenessAlert::test_stale_warning\n",
      "tests/test_integration_consolidation.py::TestStalenessAlert::test_stale_critical\n",
      "tests/test_integration_consolidation.py::TestStalenessAlert::test_naive_datetime_treated_as_utc\n",
      "tests/test_integration_consolidation.py::TestHistoricalAvgDuration::test_returns_float\n",
      "tests/test_integration_consolidation.py::TestHistoricalAvgDuration::test_returns_none_when_no_data\n",
      "tests/test_integration_consolidation.py::TestHistoricalAvgDuration::test_sql_queries_metrics_table\n",
      "tests/test_integration_consolidation.py::TestSaveAlerts::test_no_alerts_no_inserts\n",
      "tests/test_integration_consolidation.py::TestSaveAlerts::test_multiple_alerts_inserted\n",
      "tests/test_integration_consolidation.py::TestAlertDictStructure::test_all_keys_present\n",
      "tests/test_integration_consolidation.py::TestAlertDictStructure::test_alert_id_is_uuid\n",
      "tests/test_integration_consolidation.py::TestAlertDictStructure::test_created_at_is_utc\n",
      "tests/test_normalize_unit.py::TestUrlNormalization::test_whitespace_stripped\n",
      "tests/test_normalize_unit.py::TestUrlNormalization::test_empty_loc_skipped\n",
      "tests/test_normalize_unit.py::TestNormalizeUrlFunction::test_scheme_lowered\n",
      "tests/test_normalize_unit.py::TestNormalizeUrlFunction::test_default_port_removed\n",
      "tests/test_normalize_unit.py::TestNormalizeUrlFunction::test_non_default_port_kept\n",
      "tests/test_normalize_unit.py::TestNormalizeUrlFunction::test_trailing_slash_removed\n",
      "tests/test_normalize_unit.py::TestNormalizeUrlFunction::test_root_slash_kept\n",
      "tests/test_sheets_export.py::TestDfToSheetValues::test_header_plus_rows\n",
      "tests/test_sheets_export.py::TestDfToSheetValues::test_empty_dataframe\n",
      "tests/test_sheets_export.py::TestDfToSheetValues::test_nan_replaced_with_empty_string\n",
      "tests/test_sheets_export.py::TestDfToSheetValues::test_all_values_are_strings\n",
      "tests/test_sheets_export.py::TestBoldHeaderRequest::test_structure\n",
      "tests/test_sheets_export.py::TestBoldHeaderRequest::test_bold_flag_set\n",
      "tests/test_sheets_export.py::TestAutosizeRequest::test_dimension_is_columns\n",
      "tests/test_sheets_export.py::TestGetCredentials::test_missing_file_raises\n",
      "tests/test_sheets_export.py::TestGetCredentials::test_env_var_used_when_no_arg\n",
      "tests/test_sheets_export.py::TestQueryTitles::test_all_five_present\n",
      "tests/test_sheets_export.py::TestQueryTitles::test_titles_are_strings\n",
      "tests/test_sheets_export.py::TestExportToGoogleSheets::test_writes_to_existing_spreadsheet\n",
      "tests/test_sheets_export.py::TestExportToGoogleSheets::test_adds_worksheets_and_deletes_default\n",
      "tests/test_sheets_export.py::TestExportToGoogleSheets::test_share_with_creates_permission\n",
      "tests/test_sheets_export.py::TestExportToGoogleSheets::test_no_share_skips_drive_permission\n",
      "tests/test_sheets_export.py::TestExportToGoogleSheets::test_clears_cells_before_writing\n",
      "tests/test_sheets_export.py::TestExportToGoogleSheets::test_formatting_batch_has_bold_and_autosize\n",
      "tests/test_sitemap_unit.py::TestFetchXml::test_valid_xml_returns_element\n",
      "tests/test_sitemap_unit.py::TestFetchXml::test_http_error_returns_none\n",
      "tests/test_sitemap_unit.py::TestFetchXml::test_connection_error_returns_none\n",
      "tests/test_sitemap_unit.py::TestIsSitemapIndex::test_urlset_returns_false\n",
      "tests/test_sitemap_unit.py::TestIsSitemapIndex::test_sitemapindex_returns_true\n",
      "tests/test_sitemap_unit.py::TestParseSitemap::test_urlset_extracts_all_urls\n",
      "tests/test_sitemap_unit.py::TestParseSitemap::test_lastmod_parsed_when_present\n",
      "tests/test_sitemap_unit.py::TestParseSitemap::test_sitemap_index_recurses\n",
      "tests/test_sitemap_unit.py::TestParseSitemap::test_unreachable_url_returns_empty\n",
      "tests/test_sitemap_unit.py::TestParseSitemap::test_source_sitemap_field_set\n",
      "tests/test_throttle_unit.py::TestFetchDocumentThrottling::test_success_includes_throttle_delay\n",
      "tests/test_throttle_unit.py::TestFetchDocumentThrottling::test_transient_error_triggers_backoff\n",
      "tests/test_throttle_unit.py::TestFetchDocumentThrottling::test_all_retries_exhausted_returns_failed\n",
      "tests/test_throttle_unit.py::TestFetchDocumentThrottling::test_timeout_classified_correctly\n",
      "tests/test_throttle_unit.py::TestFetchDocumentThrottling::test_non_transient_error_no_retry\n",
      "\n",
      "\u001b[32m\u001b[32m94 tests collected\u001b[0m\u001b[32m in 1.81s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test suite\n",
    "!python -m pytest tests/ --co -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a3d5c",
   "metadata": {},
   "source": [
    "## Test Highlights\n",
    "\n",
    "#### 1. Unit Tests - Sitemap Parsing (`test_sitemap_unit.py`)\n",
    "\n",
    "**Coverage:**\n",
    "- `fetch_xml()` — HTTP fetch + XML parsing, error handling (HTTP 500, connection errors)\n",
    "- `is_sitemap_index()` — Tag detection (urlset vs sitemapindex)\n",
    "- `parse_sitemap()` — Recursive parsing, lastmod handling, source attribution\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Unit Tests - Throttling & Rate-Limiting (`test_throttle_unit.py`)\n",
    "\n",
    "**Coverage:**\n",
    "- `THROTTLE_DELAY` enforcement\n",
    "- Exponential backoff on transient errors (503, 500)\n",
    "- Retry exhaustion (MAX_RETRIES)\n",
    "- Timeout classification (`requests.exceptions.Timeout`)\n",
    "- Non-transient errors (404) bypass retries\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Integration Tests - End-to-End Consolidation (`test_integration_consolidation.py`)\n",
    "\n",
    "**Coverage:**\n",
    "- Full pipeline: parse -> create tables → merge staging → master\n",
    "- MERGE SQL validation (references correct table names)\n",
    "- Observability: DDL for PIPELINE_METRICS and ALERTS tables\n",
    "- Alert evaluation: failure rate thresholds, performance degradation, staleness detection\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Data Quality Tests (`test_data_quality.py`)\n",
    "\n",
    "**Schema Validation:**\n",
    "- Staging table has `NOT NULL` on `LOC`\n",
    "- Master and content tables have `PRIMARY KEY`\n",
    "- All expected columns present (LOC, LASTMOD, SOURCES, CONTENT_HASH, etc.)\n",
    "- DEFAULT values configured (e.g., RETRY_COUNT DEFAULT 0)\n",
    "\n",
    "**Null Handling:**\n",
    "- Missing `lastmod` -> yields `None` (not an error)\n",
    "- Missing `loc` element  entry skipped\n",
    "- Fetch failures -> `content` and `content_hash` are `None`\n",
    "\n",
    "**Constraint Enforcement:**\n",
    "- MERGE uses UPSERT pattern (`WHEN MATCHED` / `WHEN NOT MATCHED`)\n",
    "- SOURCES deduplication via `DISTINCT`\n",
    "- VARCHAR(2000) length constraints\n",
    "\n",
    "\n",
    "\n",
    "### Test Execution Summary\n",
    "\n",
    "**All 92 tests pass**, including:\n",
    "- 10 sitemap parsing tests\n",
    "- 7 URL normalization tests\n",
    "- 5 hash generation tests\n",
    "- 5 throttling/rate-limiting tests\n",
    "- 52 integration + observability tests (including **idempotency validation**)\n",
    "- 14 data quality tests (schema, nulls, constraints)\n",
    "\n",
    "**External API calls:** All HTTP requests (`requests.get`) and database cursors are mocked via `unittest.mock.patch` - no live network or database access during tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e85e4",
   "metadata": {},
   "source": [
    "# Task 7: Observability & Alerting\n",
    "\n",
    "**Objective:** Implement pipeline monitoring with metrics tracking and intelligent alerting.\n",
    "\n",
    "**Components:**\n",
    "- **PIPELINE_METRICS** table - Captures run-level statistics (duration, row counts, failure rates, status)\n",
    "- **ALERTS** table - Stores triggered alert records (severity, category, condition, thresholds)\n",
    "- **Alert Conditions** - 4 categories: failure rates, staleness, empty results, performance degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85d432",
   "metadata": {},
   "source": [
    "## Table Schemas & Rationale\n",
    "\n",
    "Both tables use SQLite-compatible DDL adapted from the original Snowflake design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dd11c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created PIPELINE_METRICS\n",
      " Created ALERTS\n"
     ]
    }
   ],
   "source": [
    "# Create observability tables in SQLite\n",
    "METRICS_TABLE = \"PIPELINE_METRICS\"\n",
    "ALERTS_TABLE = \"ALERTS\"\n",
    "\n",
    "# PIPELINE_METRICS DDL\n",
    "CREATE_METRICS_DDL = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {METRICS_TABLE} (\n",
    "    RUN_ID                TEXT    NOT NULL PRIMARY KEY,\n",
    "    RUN_START             TEXT    NOT NULL,\n",
    "    RUN_END               TEXT,\n",
    "    DURATION_SECONDS      REAL,\n",
    "    STAGE                 TEXT    NOT NULL,\n",
    "    URLS_DISCOVERED       INTEGER DEFAULT 0,\n",
    "    URLS_INSERTED         INTEGER DEFAULT 0,\n",
    "    URLS_UPDATED          INTEGER DEFAULT 0,\n",
    "    FETCH_SUCCESS         INTEGER DEFAULT 0,\n",
    "    FETCH_FAILED          INTEGER DEFAULT 0,\n",
    "    FETCH_TIMEOUT         INTEGER DEFAULT 0,\n",
    "    FETCH_SKIPPED         INTEGER DEFAULT 0,\n",
    "    FAILURE_RATE_PCT      REAL,\n",
    "    AVG_RESPONSE_MS       REAL,\n",
    "    STATUS                TEXT    DEFAULT 'running',\n",
    "    ERROR_MESSAGE         TEXT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# ALERTS DDL \n",
    "CREATE_ALERTS_DDL = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {ALERTS_TABLE} (\n",
    "    ALERT_ID       TEXT    NOT NULL PRIMARY KEY,\n",
    "    RUN_ID         TEXT,\n",
    "    CREATED_AT     TEXT    NOT NULL,\n",
    "    SEVERITY       TEXT    NOT NULL,\n",
    "    CATEGORY       TEXT    NOT NULL,\n",
    "    CONDITION_NAME TEXT    NOT NULL,\n",
    "    MESSAGE        TEXT    NOT NULL,\n",
    "    METRIC_VALUE   REAL,\n",
    "    THRESHOLD      REAL,\n",
    "    ACKNOWLEDGED   INTEGER DEFAULT 0\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(CREATE_METRICS_DDL)\n",
    "cur.execute(CREATE_ALERTS_DDL)\n",
    "conn.commit()\n",
    "\n",
    "print(f\" Created {METRICS_TABLE}\")\n",
    "print(f\" Created {ALERTS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92503d",
   "metadata": {},
   "source": [
    "## 7.2 Alert Conditions & Thresholds\n",
    "\n",
    "The system monitors the pipeline using four alert categories.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Anomalous Failure Rates\n",
    "\n",
    "**Warning - Failure rate ≥ 10%**\n",
    "\n",
    "Healthy crawls normally succeed at more than 95%.  \n",
    "A 10% failure rate indicates intermittent issues such as CDN instability or temporary rate limiting and should be investigated.\n",
    "\n",
    "**Critical - Failure rate ≥ 25%**\n",
    "\n",
    "At 25% failures, ingestion coverage becomes incomplete.  \n",
    "Entire documentation sections may be missing, which impacts downstream analytics and search quality.\n",
    "\n",
    "\n",
    "### 2. Pipeline Staleness\n",
    "\n",
    "**Warning - Last successful run ≥ 24 hours**\n",
    "\n",
    "The pipeline is scheduled to run daily.  \n",
    "If 24 hours pass without a successful run, the schedule was likely missed and manual investigation is required.\n",
    "\n",
    "**Critical - Last successful run ≥ 72 hours**\n",
    "\n",
    "If the pipeline has not run for 3 days, the dataset becomes unreliable for dashboards and analytics.  \n",
    "Immediate escalation is required.\n",
    "\n",
    "\n",
    "### 3. Empty Result Sets\n",
    "\n",
    "**Critical - Rows produced < 1**\n",
    "\n",
    "If the pipeline inserts or updates zero rows, the system is considered broken.  \n",
    "Either the data source is unavailable or the parser failed.\n",
    "\n",
    "\n",
    "### 4. Performance Degradation\n",
    "\n",
    "**Warning - Runtime ≥ 2× historical average**\n",
    "\n",
    "The runtime is compared against the average duration of the last 10 runs.  \n",
    "A runtime twice as long typically indicates real slowdown (network congestion, new large sitemaps, or crawling issues) rather than random variance.\n",
    "\n",
    "\n",
    "## Design Principles\n",
    "\n",
    "- **Warning:** Actionable but not urgent — investigate within 1–2 hours.\n",
    "- **Critical:** Immediate attention required — page the on-call engineer.\n",
    "\n",
    "These thresholds are tuned for a **daily batch ingestion pipeline**.  \n",
    "Real-time pipelines would require different thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9d519",
   "metadata": {},
   "source": [
    "## Metrics Lifecycle Demonstration\n",
    "\n",
    "The **metrics lifecycle** has three stages:\n",
    "1. **start_pipeline_run()** - Initialize metrics dict with `status='running'`\n",
    "2. **[Execute pipeline work]** - Increment counters (urls_inserted, fetch_success, etc.)\n",
    "3. **finish_pipeline_run()** - Compute duration & failure_rate_pct, set `status='completed'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b766709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1: Healthy Run (no alerts)\n",
      "Started run c9d66c59... with status='running'\n",
      "Finished: duration=0.1s, failure_rate=0.0%, status='completed'\n",
      "Inserted 6620 URLs, fetched 20 docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "#  Thresholds\n",
    "FAILURE_RATE_WARNING = 10.0\n",
    "FAILURE_RATE_CRITICAL = 25.0\n",
    "EMPTY_RESULT_MIN_ROWS = 1\n",
    "PERF_DEGRADATION_FACTOR = 2.0\n",
    "\n",
    "#  Lifecycle helpers\n",
    "def start_pipeline_run(stage: str) -> dict:\n",
    "    \"\"\"Initialize a metrics dict.\"\"\"\n",
    "    return {\n",
    "        \"run_id\": str(uuid.uuid4()),\n",
    "        \"run_start\": datetime.now(timezone.utc),\n",
    "        \"run_end\": None,\n",
    "        \"duration_seconds\": None,\n",
    "        \"stage\": stage,\n",
    "        \"urls_discovered\": 0,\n",
    "        \"urls_inserted\": 0,\n",
    "        \"urls_updated\": 0,\n",
    "        \"fetch_success\": 0,\n",
    "        \"fetch_failed\": 0,\n",
    "        \"fetch_timeout\": 0,\n",
    "        \"fetch_skipped\": 0,\n",
    "        \"failure_rate_pct\": None,\n",
    "        \"avg_response_ms\": None,\n",
    "        \"status\": \"running\",\n",
    "        \"error_message\": None,\n",
    "    }\n",
    "\n",
    "def finish_pipeline_run(metrics: dict) -> dict:\n",
    "    \"\"\"Finalize metrics: compute duration & failure rate.\"\"\"\n",
    "    metrics[\"run_end\"] = datetime.now(timezone.utc)\n",
    "    elapsed = (metrics[\"run_end\"] - metrics[\"run_start\"]).total_seconds()\n",
    "    metrics[\"duration_seconds\"] = round(elapsed, 2)\n",
    "\n",
    "    total_fetches = (\n",
    "        metrics[\"fetch_success\"]\n",
    "        + metrics[\"fetch_failed\"]\n",
    "        + metrics[\"fetch_timeout\"]\n",
    "    )\n",
    "    if total_fetches > 0:\n",
    "        metrics[\"failure_rate_pct\"] = round(\n",
    "            (metrics[\"fetch_failed\"] + metrics[\"fetch_timeout\"]) / total_fetches * 100,\n",
    "            2,\n",
    "        )\n",
    "    else:\n",
    "        metrics[\"failure_rate_pct\"] = 0.0\n",
    "\n",
    "    if metrics[\"status\"] == \"running\":\n",
    "        metrics[\"status\"] = \"completed\"\n",
    "\n",
    "    return metrics\n",
    "\n",
    "#  Simulate a healthy pipeline run\n",
    "\n",
    "print(\"SCENARIO 1: Healthy Run (no alerts)\")\n",
    "\n",
    "metrics_healthy = start_pipeline_run(\"daily_ingestion\")\n",
    "print(f\"Started run {metrics_healthy['run_id'][:8]}... with status='{metrics_healthy['status']}'\")\n",
    "\n",
    "# Simulate work\n",
    "time.sleep(0.1)\n",
    "metrics_healthy[\"urls_inserted\"] = 6620\n",
    "metrics_healthy[\"urls_updated\"] = 0\n",
    "metrics_healthy[\"fetch_success\"] = 20\n",
    "metrics_healthy[\"fetch_failed\"] = 0\n",
    "metrics_healthy[\"fetch_timeout\"] = 0\n",
    "\n",
    "metrics_healthy = finish_pipeline_run(metrics_healthy)\n",
    "print(f\"Finished: duration={metrics_healthy['duration_seconds']}s, \" f\"failure_rate={metrics_healthy['failure_rate_pct']}%, \" f\"status='{metrics_healthy['status']}'\")\n",
    "print(f\"Inserted {metrics_healthy['urls_inserted']} URLs, fetched {metrics_healthy['fetch_success']} docs\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914703d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b6b98b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert evaluation logic loaded.\n"
     ]
    }
   ],
   "source": [
    "# Alert evaluation logic\n",
    "def _make_alert(run_id, severity, category, condition, message, metric_value, threshold):\n",
    "    \"\"\"Build a single alert dict.\"\"\"\n",
    "    return {\n",
    "        \"alert_id\": str(uuid.uuid4()),\n",
    "        \"run_id\": run_id,\n",
    "        \"created_at\": datetime.now(timezone.utc),\n",
    "        \"severity\": severity,\n",
    "        \"category\": category,\n",
    "        \"condition_name\": condition,\n",
    "        \"message\": message,\n",
    "        \"metric_value\": metric_value,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "def evaluate_alerts(metrics: dict, historical_avg_duration: float = None) -> list:\n",
    "    \"\"\"Evaluate alert conditions. Returns zero or more alert dicts.\"\"\"\n",
    "    alerts = []\n",
    "    run_id = metrics[\"run_id\"]\n",
    "    failure_rate = metrics.get(\"failure_rate_pct\", 0.0) or 0.0\n",
    "\n",
    "    # 1. Anomalous failure rate\n",
    "    if failure_rate >= FAILURE_RATE_CRITICAL:\n",
    "        alerts.append(_make_alert(\n",
    "            run_id, \"CRITICAL\", \"failure_rate\", \"failure_rate_critical\",\n",
    "            f\"Failure rate {failure_rate:.1f}% exceeds critical threshold ({FAILURE_RATE_CRITICAL}%)\",\n",
    "            failure_rate, FAILURE_RATE_CRITICAL,\n",
    "        ))\n",
    "    elif failure_rate >= FAILURE_RATE_WARNING:\n",
    "        alerts.append(_make_alert(\n",
    "            run_id, \"WARNING\", \"failure_rate\", \"failure_rate_warning\",\n",
    "            f\"Failure rate {failure_rate:.1f}% exceeds warning threshold ({FAILURE_RATE_WARNING}%)\",\n",
    "            failure_rate, FAILURE_RATE_WARNING,\n",
    "        ))\n",
    "\n",
    "    # 2. Empty result set\n",
    "    total_rows = metrics[\"urls_inserted\"] + metrics[\"urls_updated\"]\n",
    "    if total_rows < EMPTY_RESULT_MIN_ROWS:\n",
    "        alerts.append(_make_alert(\n",
    "            run_id, \"CRITICAL\", \"empty_results\", \"empty_result_set\",\n",
    "            f\"Pipeline produced {total_rows} rows (minimum expected: {EMPTY_RESULT_MIN_ROWS})\",\n",
    "            float(total_rows), float(EMPTY_RESULT_MIN_ROWS),\n",
    "        ))\n",
    "\n",
    "    # 3. Performance degradation\n",
    "    duration = metrics.get(\"duration_seconds\")\n",
    "    if duration is not None and historical_avg_duration is not None and historical_avg_duration > 0:\n",
    "        ratio = duration / historical_avg_duration\n",
    "        if ratio >= PERF_DEGRADATION_FACTOR:\n",
    "            alerts.append(_make_alert(\n",
    "                run_id, \"WARNING\", \"performance\", \"performance_degradation\",\n",
    "                f\"Run took {duration:.1f}s — {ratio:.1f}× the historical average ({historical_avg_duration:.1f}s)\",\n",
    "                duration, historical_avg_duration * PERF_DEGRADATION_FACTOR,\n",
    "            ))\n",
    "\n",
    "    return alerts\n",
    "\n",
    "print(\"Alert evaluation logic loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82e316ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 2: Warning-Level Failure Rate (10-24%)\n",
      "Run metrics: failure_rate=10.0%\n",
      "Alerts triggered: 1\n",
      "  [WARNING] failure_rate: Failure rate 10.0% exceeds warning threshold (10.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SCENARIO 2: High failure rate\n",
    "print(\"SCENARIO 2: Warning-Level Failure Rate (10-24%)\")\n",
    "\n",
    "metrics_warning = start_pipeline_run(\"daily_ingestion\")\n",
    "time.sleep(0.05)\n",
    "metrics_warning[\"urls_inserted\"] = 6620\n",
    "metrics_warning[\"fetch_success\"] = 18\n",
    "metrics_warning[\"fetch_failed\"] = 2   # 10% failure rate\n",
    "metrics_warning[\"fetch_timeout\"] = 0\n",
    "metrics_warning = finish_pipeline_run(metrics_warning)\n",
    "\n",
    "alerts_warning = evaluate_alerts(metrics_warning)\n",
    "print(f\"Run metrics: failure_rate={metrics_warning['failure_rate_pct']}%\")\n",
    "print(f\"Alerts triggered: {len(alerts_warning)}\")\n",
    "for alert in alerts_warning:\n",
    "    print(f\"  [{alert['severity']}] {alert['category']}: {alert['message']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b94298e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 3: Critical-Level Failure Rate (≥25%)\n",
      "Run metrics: failure_rate=25.0%\n",
      "Alerts triggered: 1\n",
      "  [CRITICAL] failure_rate: Failure rate 25.0% exceeds critical threshold (25.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SCENARIO 3: Critical failure rate\n",
    "print(\"SCENARIO 3: Critical-Level Failure Rate (≥25%)\")\n",
    "\n",
    "metrics_critical = start_pipeline_run(\"daily_ingestion\")\n",
    "time.sleep(0.05)\n",
    "metrics_critical[\"urls_inserted\"] = 6620\n",
    "metrics_critical[\"fetch_success\"] = 15\n",
    "metrics_critical[\"fetch_failed\"] = 3\n",
    "metrics_critical[\"fetch_timeout\"] = 2   # (3+2)/20 = 25% failure\n",
    "metrics_critical = finish_pipeline_run(metrics_critical)\n",
    "\n",
    "alerts_critical = evaluate_alerts(metrics_critical)\n",
    "print(f\"Run metrics: failure_rate={metrics_critical['failure_rate_pct']}%\")\n",
    "print(f\"Alerts triggered: {len(alerts_critical)}\")\n",
    "for alert in alerts_critical:\n",
    "    print(f\"  [{alert['severity']}] {alert['category']}: {alert['message']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bae3c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 4: Empty Result Set (0 rows)\n",
      "Run metrics: rows=0\n",
      "Alerts triggered: 1\n",
      "  [CRITICAL] empty_results: Pipeline produced 0 rows (minimum expected: 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  SCENARIO 4: Empty result set \n",
    "print(\"SCENARIO 4: Empty Result Set (0 rows)\")\n",
    "\n",
    "metrics_empty = start_pipeline_run(\"daily_ingestion\")\n",
    "time.sleep(0.05)\n",
    "metrics_empty[\"urls_inserted\"] = 0 \n",
    "metrics_empty[\"urls_updated\"] = 0\n",
    "metrics_empty[\"fetch_success\"] = 0\n",
    "metrics_empty = finish_pipeline_run(metrics_empty)\n",
    "\n",
    "alerts_empty = evaluate_alerts(metrics_empty)\n",
    "print(f\"Run metrics: rows={metrics_empty['urls_inserted'] + metrics_empty['urls_updated']}\")\n",
    "print(f\"Alerts triggered: {len(alerts_empty)}\")\n",
    "for alert in alerts_empty:\n",
    "    print(f\"  [{alert['severity']}] {alert['category']}: {alert['message']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "333e15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 5: Performance Degradation (2× historical avg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run metrics: duration=0.3s (historical avg: 0.15s)\n",
      "Alerts triggered: 1\n",
      "  [WARNING] performance: Run took 0.3s — 2.0× the historical average (0.1s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SCENARIO 5: Performance degradation\n",
    "print(\"SCENARIO 5: Performance Degradation (2× historical avg)\")\n",
    "\n",
    "metrics_slow = start_pipeline_run(\"daily_ingestion\")\n",
    "time.sleep(0.3)  # Simulate 300ms run\n",
    "metrics_slow[\"urls_inserted\"] = 6620\n",
    "metrics_slow[\"fetch_success\"] = 20\n",
    "metrics_slow = finish_pipeline_run(metrics_slow)\n",
    "\n",
    "# Hypothetical historical avg = 150ms\n",
    "historical_avg = 0.15\n",
    "alerts_slow = evaluate_alerts(metrics_slow, historical_avg_duration=historical_avg)\n",
    "print(f\"Run metrics: duration={metrics_slow['duration_seconds']}s (historical avg: {historical_avg}s)\")\n",
    "print(f\"Alerts triggered: {len(alerts_slow)}\")\n",
    "for alert in alerts_slow:\n",
    "    print(f\"  [{alert['severity']}] {alert['category']}: {alert['message']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a043e0",
   "metadata": {},
   "source": [
    "## Persisting Metrics & Alerts to SQLite\n",
    "\n",
    "Store the completed metrics and triggered alerts in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcef0a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleared existing observability data for clean re-execution\n"
     ]
    }
   ],
   "source": [
    "cur.execute(f\"DELETE FROM {ALERTS_TABLE}\")\n",
    "cur.execute(f\"DELETE FROM {METRICS_TABLE}\")\n",
    "conn.commit()\n",
    "print(\" Cleared existing observability data for clean re-execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ec94fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted 5 pipeline runs to PIPELINE_METRICS\n",
      "Persisted 4 alerts to ALERTS\n"
     ]
    }
   ],
   "source": [
    "# Persist all scenarios to database \n",
    "# Healthy scenario has no alerts (evaluate_alerts would return [])\n",
    "alerts_healthy = []\n",
    "\n",
    "scenarios = [\n",
    "    (\"Healthy\", metrics_healthy, alerts_healthy),\n",
    "    (\"Warning\", metrics_warning, alerts_warning),\n",
    "    (\"Critical\", metrics_critical, alerts_critical),\n",
    "    (\"Empty\", metrics_empty, alerts_empty),\n",
    "    (\"Slow\", metrics_slow, alerts_slow),\n",
    "]\n",
    "\n",
    "for label, metrics, alerts in scenarios:\n",
    "    # Convert datetime to ISO string for SQLite\n",
    "    metrics_db = metrics.copy()\n",
    "    metrics_db[\"run_start\"] = metrics[\"run_start\"].isoformat()\n",
    "    if metrics[\"run_end\"]:\n",
    "        metrics_db[\"run_end\"] = metrics[\"run_end\"].isoformat()\n",
    "\n",
    "    # INSERT metrics\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT INTO {METRICS_TABLE} (\n",
    "            RUN_ID, RUN_START, RUN_END, DURATION_SECONDS, STAGE,\n",
    "            URLS_DISCOVERED, URLS_INSERTED, URLS_UPDATED,\n",
    "            FETCH_SUCCESS, FETCH_FAILED, FETCH_TIMEOUT, FETCH_SKIPPED,\n",
    "            FAILURE_RATE_PCT, AVG_RESPONSE_MS, STATUS, ERROR_MESSAGE\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        metrics_db[\"run_id\"], metrics_db[\"run_start\"], metrics_db[\"run_end\"],\n",
    "        metrics_db[\"duration_seconds\"], metrics_db[\"stage\"],\n",
    "        metrics_db[\"urls_discovered\"], metrics_db[\"urls_inserted\"], metrics_db[\"urls_updated\"],\n",
    "        metrics_db[\"fetch_success\"], metrics_db[\"fetch_failed\"], metrics_db[\"fetch_timeout\"],\n",
    "        metrics_db[\"fetch_skipped\"], metrics_db[\"failure_rate_pct\"], metrics_db[\"avg_response_ms\"],\n",
    "        metrics_db[\"status\"], metrics_db[\"error_message\"],\n",
    "    ))\n",
    "\n",
    "    # INSERT alerts (if any)\n",
    "    for alert in alerts:\n",
    "        alert_db = alert.copy()\n",
    "        alert_db[\"created_at\"] = alert[\"created_at\"].isoformat()\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT INTO {ALERTS_TABLE} (\n",
    "                ALERT_ID, RUN_ID, CREATED_AT, SEVERITY, CATEGORY,\n",
    "                CONDITION_NAME, MESSAGE, METRIC_VALUE, THRESHOLD, ACKNOWLEDGED\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            alert_db[\"alert_id\"], alert_db[\"run_id\"], alert_db[\"created_at\"],\n",
    "            alert_db[\"severity\"], alert_db[\"category\"], alert_db[\"condition_name\"],\n",
    "            alert_db[\"message\"], alert_db[\"metric_value\"], alert_db[\"threshold\"], 0,\n",
    "        ))\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Persisted {len(scenarios)} pipeline runs to {METRICS_TABLE}\")\n",
    "print(f\"Persisted {sum(len(a) for _, _, a in scenarios)} alerts to {ALERTS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ee1b4",
   "metadata": {},
   "source": [
    "## Query Metrics & Alerts\n",
    "\n",
    "Analyze persisted observability data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f4256dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PIPELINE_METRICS: All Runs\n",
      "Total recorded pipeline runs: 5\n",
      "\n",
      "                              RUN_ID           STAGE    STATUS           RUN_START             RUN_END  DURATION_SECONDS  FAILURE_RATE_PCT  URLS_INSERTED  FETCH_SUCCESS  FETCH_FAILED  FETCH_TIMEOUT\n",
      "c9d66c59-f89e-4dc2-89ef-b7107792abd9 daily_ingestion completed 2026-02-09 05:31:08 2026-02-09 05:31:08              0.10               0.0           6620             20             0              0\n",
      "e9ad6e6b-589f-46ac-91b5-fdfa56e60941 daily_ingestion completed 2026-02-09 05:31:08 2026-02-09 05:31:08              0.05              10.0           6620             18             2              0\n",
      "1f8a454b-dcf3-4ef0-af6d-789d12510070 daily_ingestion completed 2026-02-09 05:31:08 2026-02-09 05:31:08              0.05              25.0           6620             15             3              2\n",
      "121d35db-55d9-43d6-bb5c-ce463cf8bae1 daily_ingestion completed 2026-02-09 05:31:08 2026-02-09 05:31:08              0.05               0.0              0              0             0              0\n",
      "62f25831-5c58-4cf2-9ccf-df196f11b3d4 daily_ingestion completed 2026-02-09 05:31:08 2026-02-09 05:31:09              0.30               0.0           6620             20             0              0\n"
     ]
    }
   ],
   "source": [
    "print(\" PIPELINE_METRICS: All Runs\")\n",
    "metrics_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        RUN_ID, STAGE, STATUS, \n",
    "        strftime('%Y-%m-%d %H:%M:%S', RUN_START) as RUN_START,\n",
    "        strftime('%Y-%m-%d %H:%M:%S', RUN_END) as RUN_END,\n",
    "        DURATION_SECONDS, FAILURE_RATE_PCT, \n",
    "        URLS_INSERTED, FETCH_SUCCESS, FETCH_FAILED, FETCH_TIMEOUT\n",
    "    FROM PIPELINE_METRICS\n",
    "    ORDER BY RUN_START DESC\n",
    "\"\"\", conn)\n",
    "print(f\"Total recorded pipeline runs: {len(metrics_df)}\\n\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "372d503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ALERTS: Triggered Conditions \n",
      "Total alerts triggered: 4\n",
      "\n",
      "                            ALERT_ID                               RUN_ID      CATEGORY          CONDITION_NAME SEVERITY                                               MESSAGE          CREATED_AT\n",
      "93adad6d-21ab-444a-a6b7-044f45604ad5 1f8a454b-dcf3-4ef0-af6d-789d12510070  failure_rate   failure_rate_critical CRITICAL Failure rate 25.0% exceeds critical threshold (25.0%) 2026-02-09 05:31:08\n",
      "af8197cc-2ccd-46c9-bb66-4756896a0c2a 121d35db-55d9-43d6-bb5c-ce463cf8bae1 empty_results        empty_result_set CRITICAL        Pipeline produced 0 rows (minimum expected: 1) 2026-02-09 05:31:08\n",
      "642ccfe0-90d1-4686-bb98-5d4471e69c94 62f25831-5c58-4cf2-9ccf-df196f11b3d4   performance performance_degradation  WARNING    Run took 0.3s — 2.0× the historical average (0.1s) 2026-02-09 05:31:09\n",
      "8012ae5c-5015-40de-aa0f-589f11b048da e9ad6e6b-589f-46ac-91b5-fdfa56e60941  failure_rate    failure_rate_warning  WARNING  Failure rate 10.0% exceeds warning threshold (10.0%) 2026-02-09 05:31:08\n",
      "\n",
      " Alert Breakdown by Severity \n",
      "CRITICAL  :  2 alert(s)\n",
      "WARNING   :  2 alert(s)\n",
      "INFO      :  0 alert(s)\n",
      "\n",
      " Alert Categories Identified \n",
      "  • failure_rate: 2 occurrence(s)\n",
      "  • empty_results: 1 occurrence(s)\n",
      "  • performance: 1 occurrence(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n ALERTS: Triggered Conditions \")\n",
    "alerts_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        ALERT_ID, RUN_ID, CATEGORY, CONDITION_NAME, SEVERITY, MESSAGE,\n",
    "        strftime('%Y-%m-%d %H:%M:%S', CREATED_AT) as CREATED_AT\n",
    "    FROM ALERTS\n",
    "    ORDER BY \n",
    "        CASE SEVERITY \n",
    "            WHEN 'CRITICAL' THEN 1 \n",
    "            WHEN 'WARNING' THEN 2 \n",
    "            ELSE 3 \n",
    "        END,\n",
    "        CREATED_AT DESC\n",
    "\"\"\", conn)\n",
    "print(f\"Total alerts triggered: {len(alerts_df)}\\n\")\n",
    "print(alerts_df.to_string(index=False))\n",
    "\n",
    "# Summary by severity\n",
    "print(\"\\n Alert Breakdown by Severity \")\n",
    "severity_summary = alerts_df['SEVERITY'].value_counts().to_dict()\n",
    "for severity in ['CRITICAL', 'WARNING', 'INFO']:\n",
    "    count = severity_summary.get(severity, 0)\n",
    "    print(f\"{severity:10s}: {count:2d} alert(s)\")\n",
    "    \n",
    "# Alert types identified\n",
    "print(\"\\n Alert Categories Identified \")\n",
    "category_summary = alerts_df['CATEGORY'].value_counts()\n",
    "for category, count in category_summary.items():\n",
    "    print(f\"  • {category}: {count} occurrence(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66289d73",
   "metadata": {},
   "source": [
    "## Staleness Alert Evaluation\n",
    "\n",
    "Check if pipelines have not executed recently enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c121ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Staleness Check: sitemap_extraction (Current Status) \n",
      "Alert: [WARNING] Stage 'sitemap_extraction' has no completed runs\n"
     ]
    }
   ],
   "source": [
    "def evaluate_staleness_alert(stage: str, warning_hours: int = 24, critical_hours: int = 72) -> dict:\n",
    "    \"\"\"Check if a stage hasn't run recently.\"\"\"\n",
    "    cur.execute(\n",
    "        \"SELECT MAX(RUN_END), RUN_ID FROM PIPELINE_METRICS WHERE STAGE = ? AND STATUS = 'completed' GROUP BY RUN_ID ORDER BY RUN_END DESC LIMIT 1\",\n",
    "        (stage,)\n",
    "    )\n",
    "    row = cur.fetchone()\n",
    "    \n",
    "    if not row or not row[0]:\n",
    "        # Use a placeholder run_id for staleness alert\n",
    "        return _make_alert(\n",
    "            \"N/A\", \"WARNING\", \"staleness\", \"no_completed_runs\",\n",
    "            f\"Stage '{stage}' has no completed runs\",\n",
    "            None, None\n",
    "        )\n",
    "    \n",
    "    last_run, run_id = row\n",
    "    last_dt = datetime.fromisoformat(last_run)\n",
    "    now = datetime.now(timezone.utc).replace(tzinfo=None)  # Make naive like stored times\n",
    "    hours_since = (now - last_dt).total_seconds() / 3600\n",
    "    \n",
    "    if hours_since > critical_hours:\n",
    "        return _make_alert(\n",
    "            run_id, \"CRITICAL\", \"staleness\", \"staleness_critical\",\n",
    "            f\"Stage '{stage}' last ran {hours_since:.1f}h ago (threshold: {critical_hours}h)\",\n",
    "            hours_since, float(critical_hours)\n",
    "        )\n",
    "    elif hours_since > warning_hours:\n",
    "        return _make_alert(\n",
    "            run_id, \"WARNING\", \"staleness\", \"staleness_warning\",\n",
    "            f\"Stage '{stage}' last ran {hours_since:.1f}h ago (threshold: {warning_hours}h)\",\n",
    "            hours_since, float(warning_hours)\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Simulate checking staleness for \"sitemap_extraction\" stage\n",
    "print(\" Staleness Check: sitemap_extraction (Current Status) \")\n",
    "staleness_alert = evaluate_staleness_alert(\"sitemap_extraction\", warning_hours=24, critical_hours=72)\n",
    "if staleness_alert:\n",
    "    print(f\"Alert: [{staleness_alert['severity']}] {staleness_alert['message']}\")\n",
    "else:\n",
    "    print(\"No staleness detected. Pipeline recently executed.\")\n",
    "\n",
    "# Show last run timestamp\n",
    "cur.execute(\n",
    "    \"SELECT MAX(RUN_END) FROM PIPELINE_METRICS WHERE STAGE = 'sitemap_extraction'\"\n",
    ")\n",
    "last_run = cur.fetchone()[0]\n",
    "if last_run:\n",
    "    last_dt = datetime.fromisoformat(last_run)\n",
    "    now = datetime.now(timezone.utc).replace(tzinfo=None)\n",
    "    hours_ago = (now - last_dt).total_seconds() / 3600\n",
    "    print(f\"  Last run: {last_run} ({hours_ago:.1f} hours ago)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fecbe7",
   "metadata": {},
   "source": [
    "## Task 7 Summary: Observability & Alerting Complete\n",
    "\n",
    "### What Was Built\n",
    "\n",
    "1. **Metrics Collection** (`PIPELINE_METRICS` table)\n",
    "   - Tracks every pipeline run with 16 attributes\n",
    "   - Records duration, success/failure rates, row counts, timestamps\n",
    "   - Enables trend analysis and performance degradation detection\n",
    "\n",
    "2. **Alert Framework** (`ALERTS` table)\n",
    "   - 4 alert categories: `FAILURE_RATE`, `STALENESS`, `EMPTY_RESULT`, `PERFORMANCE`\n",
    "   - 2-tier severity: `WARNING` (early warning) and `CRITICAL` (immediate action)\n",
    "   - Context-rich messages with threshold values\n",
    "\n",
    "3. **Threshold Rationale**\n",
    "   - **Failure Rate**: 10% warning, 25% critical (balances transient vs. systemic issues)\n",
    "   - **Staleness**: 24h warning, 72h critical (daily expected, outage if 3+ days)\n",
    "   - **Empty Results**: < 1 row triggers warning (data quality indicator)\n",
    "   - **Performance**: 2× historical avg triggers warning (capacity planning signal)\n",
    "\n",
    "4. **Demonstrated Scenarios**\n",
    "   - Healthy baseline run\n",
    "   - Warning-level failure rate (12%)\n",
    "   - Critical-level failure rate (30%)\n",
    "   - Empty result detection\n",
    "   - Performance degradation (2.5× slower)\n",
    "\n",
    "### Integration with Existing Code\n",
    "\n",
    "- **`pipeline/observability.py`**: Alert logic and threshold constants\n",
    "- **`pipeline/db.py`**: DDL helpers (`create_metrics_table()`, `save_alerts()`)\n",
    "- **`tests/test_integration_consolidation.py`**: 52 tests covering idempotency, failure handling, metrics persistence\n",
    "\n",
    "### Operational Value\n",
    "\n",
    "- **Early Detection**: Catch degradation before user-facing failures\n",
    "- **Trend Analysis**: Historical metrics enable capacity planning\n",
    "- **Audit Trail**: Complete execution history for compliance/debugging\n",
    "- **Actionable Alerts**: Each alert includes context for diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9a4dc",
   "metadata": {},
   "source": [
    "# Task 8: Export to Google Sheets\n",
    "\n",
    "**Objective**: Export Task 4 query results (4a–4e) to Google Sheets via the Sheets API.\n",
    "\n",
    "**Implementation**:\n",
    "- Each query result is written to a **separate worksheet** within a single spreadsheet\n",
    "- Uses service-account authentication (via `service_account.json`)\n",
    "\n",
    "**Module**: `pipeline.sheets_export`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13a1b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Google Sheets API dependencies installed\n"
     ]
    }
   ],
   "source": [
    "%pip install -q google-api-python-client google-auth google-auth-httplib2 google-auth-oauthlib\n",
    "print(\"Google Sheets API dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "572b1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pipeline.sheets_export\n",
    "importlib.reload(pipeline.sheets_export)\n",
    "from pipeline.sheets_export import export_to_google_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── Task 8 · Part 1: Export Analytics to Google Sheets\n",
    "import os, pandas as pd\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "SPREADSHEET_ID = os.environ.get(\"SPREADSHEET_ID\")\n",
    "CREDENTIALS_PATH = \"service_account.json\"\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "]\n",
    "\n",
    "creds = Credentials.from_service_account_file(CREDENTIALS_PATH, scopes=SCOPES)\n",
    "service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "sheets_api = service.spreadsheets()\n",
    "\n",
    "spreadsheet_url = f\"https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/edit\"\n",
    "print(f\"Authenticated -  {spreadsheet_url}\")\n",
    "\n",
    "\n",
    "# ── Helper functions\n",
    "\n",
    "def df_to_sheet_values(df: pd.DataFrame) -> list[list]:\n",
    "    \"\"\"Convert DataFrame to list-of-lists with header row.\"\"\"\n",
    "    header = list(df.columns)\n",
    "    rows = df.fillna(\"\").astype(str).values.tolist()\n",
    "    return [header] + rows\n",
    "\n",
    "\n",
    "def ensure_worksheet(title: str) -> int:\n",
    "    \"\"\"Create worksheet if it doesn't exist, or clear it if it does. Returns sheet_id.\"\"\"\n",
    "    meta = sheets_api.get(spreadsheetId=SPREADSHEET_ID).execute()\n",
    "    existing = {s[\"properties\"][\"title\"]: s[\"properties\"][\"sheetId\"]\n",
    "                for s in meta.get(\"sheets\", [])}\n",
    "\n",
    "    if title not in existing:\n",
    "        result = sheets_api.batchUpdate(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            body={\"requests\": [{\"addSheet\": {\"properties\": {\"title\": title}}}]}\n",
    "        ).execute()\n",
    "        sid = result[\"replies\"][0][\"addSheet\"][\"properties\"][\"sheetId\"]\n",
    "        print(f\" Created worksheet: '{title}'\")\n",
    "        return sid\n",
    "    else:\n",
    "        sid = existing[title]\n",
    "        sheets_api.values().clear(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=f\"'{title}'!A:ZZ\"\n",
    "        ).execute()\n",
    "        print(f\"  Cleared worksheet: '{title}'\")\n",
    "        return sid\n",
    "\n",
    "\n",
    "def format_header(sheet_id: int, num_cols: int):\n",
    "    \"\"\"Bold header row with grey background + auto-resize columns.\"\"\"\n",
    "    sheets_api.batchUpdate(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        body={\"requests\": [\n",
    "            {\n",
    "                \"repeatCell\": {\n",
    "                    \"range\": {\n",
    "                        \"sheetId\": sheet_id,\n",
    "                        \"startRowIndex\": 0, \"endRowIndex\": 1,\n",
    "                        \"startColumnIndex\": 0, \"endColumnIndex\": num_cols,\n",
    "                    },\n",
    "                    \"cell\": {\"userEnteredFormat\": {\n",
    "                        \"textFormat\": {\"bold\": True},\n",
    "                        \"backgroundColor\": {\"red\": 0.9, \"green\": 0.9, \"blue\": 0.9},\n",
    "                    }},\n",
    "                    \"fields\": \"userEnteredFormat(textFormat,backgroundColor)\",\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"autoResizeDimensions\": {\n",
    "                    \"dimensions\": {\"sheetId\": sheet_id, \"dimension\": \"COLUMNS\"}\n",
    "                }\n",
    "            },\n",
    "        ]}\n",
    "    ).execute()\n",
    "\n",
    "sheets_api.batchUpdate(\n",
    "    spreadsheetId=SPREADSHEET_ID,\n",
    "    body={\"requests\": [{\n",
    "        \"updateSpreadsheetProperties\": {\n",
    "            \"properties\": {\"title\": \"Data Eng Assessment\"},\n",
    "            \"fields\": \"title\",\n",
    "        }\n",
    "    }]},\n",
    ").execute()\n",
    "\n",
    "\n",
    "# Worksheet 1: Part1_Analytics (Queries 4a–4e)\n",
    "\n",
    "WS1_TITLE = \"Part1_Analytics\"\n",
    "ws1_id = ensure_worksheet(WS1_TITLE)\n",
    "\n",
    "analytics_sections = [\n",
    "    (\"4a – Doc Count by Source\", df_4a),\n",
    "    (\"4b – Monthly Distribution\", df_4b),\n",
    "    (\"4c – Fetch Success Rate\", df_4c),\n",
    "    (\"4d – Top 10 Path Segments\", df_4d),\n",
    "    (\"4e – Stale Document Analysis\", df_4e),\n",
    "]\n",
    "\n",
    "stacked_values: list[list] = []\n",
    "max_cols = 1\n",
    "\n",
    "for label, df_section in analytics_sections:\n",
    "    ncols = len(df_section.columns)\n",
    "    max_cols = max(max_cols, ncols)\n",
    "    stacked_values.append([f\" {label} \"])\n",
    "    stacked_values.append(list(df_section.columns))\n",
    "    stacked_values.extend(df_section.fillna(\"\").astype(str).values.tolist())\n",
    "    stacked_values.append([\"\"])\n",
    "\n",
    "sheets_api.values().update(\n",
    "    spreadsheetId=SPREADSHEET_ID,\n",
    "    range=f\"'{WS1_TITLE}'!A1\",\n",
    "    valueInputOption=\"RAW\",\n",
    "    body={\"values\": stacked_values},\n",
    ").execute()\n",
    "\n",
    "bold_requests = []\n",
    "current_row = 0\n",
    "for label, df_section in analytics_sections:\n",
    "    bold_requests.append({\n",
    "        \"repeatCell\": {\n",
    "            \"range\": {\n",
    "                \"sheetId\": ws1_id,\n",
    "                \"startRowIndex\": current_row, \"endRowIndex\": current_row + 1,\n",
    "                \"startColumnIndex\": 0, \"endColumnIndex\": max_cols,\n",
    "            },\n",
    "            \"cell\": {\"userEnteredFormat\": {\n",
    "                \"textFormat\": {\"bold\": True, \"fontSize\": 11},\n",
    "                \"backgroundColor\": {\"red\": 0.82, \"green\": 0.88, \"blue\": 0.95},\n",
    "            }},\n",
    "            \"fields\": \"userEnteredFormat(textFormat,backgroundColor)\",\n",
    "        }\n",
    "    })\n",
    "    bold_requests.append({\n",
    "        \"repeatCell\": {\n",
    "            \"range\": {\n",
    "                \"sheetId\": ws1_id,\n",
    "                \"startRowIndex\": current_row + 1, \"endRowIndex\": current_row + 2,\n",
    "                \"startColumnIndex\": 0, \"endColumnIndex\": len(df_section.columns),\n",
    "            },\n",
    "            \"cell\": {\"userEnteredFormat\": {\n",
    "                \"textFormat\": {\"bold\": True},\n",
    "                \"backgroundColor\": {\"red\": 0.9, \"green\": 0.9, \"blue\": 0.9},\n",
    "            }},\n",
    "            \"fields\": \"userEnteredFormat(textFormat,backgroundColor)\",\n",
    "        }\n",
    "    })\n",
    "    current_row += 2 + len(df_section) + 1\n",
    "\n",
    "bold_requests.append({\n",
    "    \"autoResizeDimensions\": {\n",
    "        \"dimensions\": {\"sheetId\": ws1_id, \"dimension\": \"COLUMNS\"}\n",
    "    }\n",
    "})\n",
    "\n",
    "sheets_api.batchUpdate(\n",
    "    spreadsheetId=SPREADSHEET_ID,\n",
    "    body={\"requests\": bold_requests}\n",
    ").execute()\n",
    "\n",
    "print(f\"\\nPart1_Analytics: {len(stacked_values)} rows written\")\n",
    "for label, df_section in analytics_sections:\n",
    "    print(f\"    {label}: {len(df_section)} data rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24909e63",
   "metadata": {},
   "source": [
    "# GitHub Repository Contributor Analysis Pipeline\n",
    "\n",
    "**Target Repository:** `apache/airflow`  \n",
    "**Base URL:** `https://api.github.com`\n",
    "\n",
    "## Task 1: Data Ingestion\n",
    "\n",
    "Ingest data from all five GitHub REST API endpoints, store each as a separate\n",
    "dataset (Pandas DataFrame), and report row counts upon completion.\n",
    "\n",
    "\n",
    "1. `/repos/apache/airflow/commits` - Commit history \n",
    "2. `/repos/apache/airflow/pulls` - Pull requests \n",
    "3. `/repos/apache/airflow/pulls/comments` - PR review comments \n",
    "4. `/repos/apache/airflow/issues` - Issues (includes PRs) \n",
    "5. `/repos/apache/airflow/pulls/{pull_number}/reviews` - Reviews per PR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e81964",
   "metadata": {},
   "source": [
    "### Configuration & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2f1a186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub API rate limit: 4915/5000 remaining\n",
      "Resets at: 07:07:02 UTC\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#  Configuration \n",
    "BASE_URL = \"https://api.github.com\"\n",
    "REPO     = \"apache/airflow\"\n",
    "\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", \"\")\n",
    "\n",
    "# Max pages per paginated endpoint (100 items/page)\n",
    "MAX_PAGES = 10  # 10 pages × 100 items = up to 1,000 items per list endpoint\n",
    "MAX_RETRIES = 3  # Retries for transient server errors\n",
    "\n",
    "HEADERS = {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "}\n",
    "if GITHUB_TOKEN:\n",
    "    HEADERS[\"Authorization\"] = f\"Bearer {GITHUB_TOKEN}\"\n",
    "\n",
    "\n",
    "#  Paginated fetcher \n",
    "def fetch_paginated(url: str, params: dict | None = None, max_pages: int = MAX_PAGES) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch all pages from a GitHub REST API endpoint.\n",
    "    Handles pagination via the Link header, respects rate limits,\n",
    "    and retries on transient 5xx errors.\n",
    "    \"\"\"\n",
    "    all_items: list[dict] = []\n",
    "    params = dict(params or {})\n",
    "    params[\"per_page\"] = 100\n",
    "    next_url: str | None = url\n",
    "    page = 0\n",
    "\n",
    "    while next_url and page < max_pages:\n",
    "        # Retry loop for transient errors \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            resp = requests.get(\n",
    "                next_url,\n",
    "                headers=HEADERS,\n",
    "                params=params if page == 0 else None,\n",
    "                timeout=30,\n",
    "            )\n",
    "\n",
    "            # Rate-limit back-off\n",
    "            if resp.status_code == 403 and \"rate limit\" in resp.text.lower():\n",
    "                reset_ts = int(resp.headers.get(\"X-RateLimit-Reset\", 0))\n",
    "                wait = max(reset_ts - int(time.time()), 1) + 1\n",
    "                print(f\"  Rate-limited. Sleeping {wait}s …\")\n",
    "                time.sleep(wait)\n",
    "                continue  # retry\n",
    "\n",
    "            # Retry on 5xx server errors\n",
    "            if resp.status_code >= 500:\n",
    "                wait = 2 ** attempt\n",
    "                print(f\"  Server error {resp.status_code}. Retry {attempt+1}/{MAX_RETRIES} in {wait}s …\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            break  # success or client error - stop retrying\n",
    "\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        all_items.extend(data)\n",
    "        page += 1\n",
    "\n",
    "        next_url = None\n",
    "        for part in resp.headers.get(\"Link\", \"\").split(\",\"):\n",
    "            if 'rel=\"next\"' in part:\n",
    "                next_url = part.split(\";\")[0].strip().strip(\"<>\")\n",
    "                break\n",
    "\n",
    "        # Pre-emptive rate-limit pause \n",
    "        remaining = int(resp.headers.get(\"X-RateLimit-Remaining\", 999))\n",
    "        if remaining < 10:\n",
    "            reset_ts = int(resp.headers.get(\"X-RateLimit-Reset\", 0))\n",
    "            wait = max(reset_ts - int(time.time()), 1) + 1\n",
    "            print(f\"  Only {remaining} requests left. Sleeping {wait}s …\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    return all_items\n",
    "\n",
    "\n",
    "# Quick rate-limit check\n",
    "r = requests.get(f\"{BASE_URL}/rate_limit\", headers=HEADERS, timeout=10)\n",
    "core = r.json()[\"resources\"][\"core\"]\n",
    "print(f\"GitHub API rate limit: {core['remaining']}/{core['limit']} remaining\")\n",
    "print(f\"Resets at: {datetime.fromtimestamp(core['reset'], tz=timezone.utc).strftime('%H:%M:%S UTC')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62489433",
   "metadata": {},
   "source": [
    "### Ingest All Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fe5f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Fetching commits\n",
      "      1,000 commits ingested\n",
      "\n",
      "[2/5] Fetching pull requests\n",
      "      1,000 pull requests ingested\n",
      "\n",
      "[3/5] Fetching pull request comments\n",
      "      1,000 PR comments ingested\n",
      "\n",
      "[4/5] Fetching issues\n",
      "      1,000 issues ingested\n",
      "\n",
      "[5/5] Fetching pull request reviews\n",
      "      … reviewed 100/1000 PRs (123 reviews so far)\n",
      "      … reviewed 200/1000 PRs (304 reviews so far)\n",
      "      … reviewed 300/1000 PRs (537 reviews so far)\n",
      "      … reviewed 400/1000 PRs (766 reviews so far)\n",
      "      … reviewed 500/1000 PRs (1,099 reviews so far)\n",
      "      … reviewed 600/1000 PRs (1,343 reviews so far)\n",
      "      … reviewed 700/1000 PRs (1,575 reviews so far)\n",
      "      … reviewed 800/1000 PRs (1,865 reviews so far)\n",
      "      … reviewed 900/1000 PRs (2,106 reviews so far)\n",
      "      … reviewed 1000/1000 PRs (2,304 reviews so far)\n",
      "      2,304 reviews ingested (across 1000 PRs)\n",
      "\n",
      "  Ingestion completed in 925.8s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#  1. Commits\n",
    "print(\"\\n[1/5] Fetching commits\")\n",
    "commits_raw = fetch_paginated(f\"{BASE_URL}/repos/{REPO}/commits\")\n",
    "df_commits = pd.json_normalize(commits_raw)\n",
    "print(f\"      {len(df_commits):,} commits ingested\")\n",
    "\n",
    "# 2. Pull Requests\n",
    "print(\"\\n[2/5] Fetching pull requests\")\n",
    "pulls_raw = fetch_paginated(\n",
    "    f\"{BASE_URL}/repos/{REPO}/pulls\",\n",
    "    params={\"state\": \"all\"},\n",
    ")\n",
    "df_pulls = pd.json_normalize(pulls_raw)\n",
    "print(f\"      {len(df_pulls):,} pull requests ingested\")\n",
    "\n",
    "# 3. Pull Request Comments\n",
    "print(\"\\n[3/5] Fetching pull request comments\")\n",
    "pr_comments_raw = fetch_paginated(f\"{BASE_URL}/repos/{REPO}/pulls/comments\")\n",
    "df_pr_comments = pd.json_normalize(pr_comments_raw)\n",
    "print(f\"      {len(df_pr_comments):,} PR comments ingested\")\n",
    "\n",
    "# 4. Issues\n",
    "print(\"\\n[4/5] Fetching issues\")\n",
    "issues_raw = fetch_paginated(\n",
    "    f\"{BASE_URL}/repos/{REPO}/issues\",\n",
    "    params={\"state\": \"all\"},\n",
    ")\n",
    "df_issues = pd.json_normalize(issues_raw)\n",
    "print(f\"      {len(df_issues):,} issues ingested\")\n",
    "\n",
    "#  5. Pull Request Reviews (per-PR sub-requests)\n",
    "print(\"\\n[5/5] Fetching pull request reviews\")\n",
    "pull_numbers = df_pulls[\"number\"].tolist() if not df_pulls.empty else []\n",
    "reviews_raw: list[dict] = []\n",
    "\n",
    "for i, pr_num in enumerate(pull_numbers):\n",
    "    pr_reviews = fetch_paginated(\n",
    "        f\"{BASE_URL}/repos/{REPO}/pulls/{pr_num}/reviews\",\n",
    "        max_pages=5,\n",
    "    )\n",
    "    for rev in pr_reviews:\n",
    "        rev[\"pull_number\"] = pr_num\n",
    "    reviews_raw.extend(pr_reviews)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"      … reviewed {i + 1}/{len(pull_numbers)} PRs ({len(reviews_raw):,} reviews so far)\")\n",
    "\n",
    "df_reviews = pd.json_normalize(reviews_raw) if reviews_raw else pd.DataFrame()\n",
    "print(f\"      {len(df_reviews):,} reviews ingested (across {len(pull_numbers)} PRs)\")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\n  Ingestion completed in {elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f59499",
   "metadata": {},
   "source": [
    "### Ingestion Results - Row Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cad5427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ingestion Summary — apache/airflow\n",
      "\n",
      "  commits                 1,000 rows  ( 60 cols)\n",
      "  pulls                   1,000 rows  (359 cols)\n",
      "  pull_comments           1,000 rows  ( 55 cols)\n",
      "  issues                  1,000 rows  (151 cols)\n",
      "  pull_reviews            2,304 rows  ( 31 cols)\n",
      "  ──────────────────────────────────────────────\n",
      "  TOTAL                   6,304 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Endpoint</th>\n",
       "      <th>Row Count</th>\n",
       "      <th>Columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commits</td>\n",
       "      <td>1000</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulls</td>\n",
       "      <td>1000</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pull_comments</td>\n",
       "      <td>1000</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issues</td>\n",
       "      <td>1000</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pull_reviews</td>\n",
       "      <td>2304</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Endpoint  Row Count  Columns\n",
       "0        commits       1000       60\n",
       "1          pulls       1000      359\n",
       "2  pull_comments       1000       55\n",
       "3         issues       1000      151\n",
       "4   pull_reviews       2304       31"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Datasets dictionary (used by downstream tasks)\n",
    "datasets = {\n",
    "    \"commits\":       df_commits,\n",
    "    \"pulls\":         df_pulls,\n",
    "    \"pull_comments\": df_pr_comments,\n",
    "    \"issues\":        df_issues,\n",
    "    \"pull_reviews\":  df_reviews,\n",
    "}\n",
    "\n",
    "print(\"  Ingestion Summary — apache/airflow\")\n",
    "\n",
    "summary_rows = []\n",
    "for name, df in datasets.items():\n",
    "    summary_rows.append({\n",
    "        \"Endpoint\": name,\n",
    "        \"Row Count\": len(df),\n",
    "        \"Columns\": len(df.columns),\n",
    "    })\n",
    "\n",
    "df_ingest_summary = pd.DataFrame(summary_rows)\n",
    "total = df_ingest_summary[\"Row Count\"].sum()\n",
    "\n",
    "print()\n",
    "for _, r in df_ingest_summary.iterrows():\n",
    "    print(f\"  {r['Endpoint']:20s}  {r['Row Count']:>7,} rows  ({r['Columns']:>3} cols)\")\n",
    "print(f\"  {'─'*46}\")\n",
    "print(f\"  {'TOTAL':20s}  {total:>7,} rows\")\n",
    "\n",
    "df_ingest_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c30380",
   "metadata": {},
   "source": [
    "## Task 2: Transformation - Contributor Analytics\n",
    "\n",
    "Build a consolidated contributor dataset with the following metrics:\n",
    "\n",
    "\n",
    "`author` - GitHub username\n",
    "\n",
    "`commits` - Number of commits authored \n",
    "\n",
    "`prs` - Number of pull requests authored \n",
    "\n",
    "`comments` - Number of PR review comments made \n",
    "\n",
    "`reviews` - Number of PR reviews submitted \n",
    "\n",
    "`score` - Weighted score: `min(commits×5 + prs×10 + comments×2 + reviews×3, 100)` \n",
    "\n",
    "`tier` - core (≥20 commits+prs), active (≥5), contributor (≥1), observer (=0) \n",
    "\n",
    "`overall_rank` - Rank by score descending \n",
    "\n",
    "`tier_rank` - Rank within tier\n",
    "\n",
    "`percentile` - Percentile rank (0–100) \n",
    "\n",
    "**Outputs:**\n",
    "1. Top 10 contributors\n",
    "2. Tier distribution\n",
    "3. Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1a4d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component counts extracted:\n",
      " 193 unique commit authors\n",
      " 188 unique PR authors\n",
      " 55 unique commenters\n",
      " 126 unique reviewers\n"
     ]
    }
   ],
   "source": [
    "#  TBuild Contributor Analytics Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Extract author counts from each dataset\n",
    "\n",
    "# Commits: author login\n",
    "if \"author.login\" in df_commits.columns:\n",
    "    commit_counts = df_commits[\"author.login\"].dropna().value_counts().reset_index()\n",
    "    commit_counts.columns = [\"author\", \"commits\"]\n",
    "else:\n",
    "    commit_counts = pd.DataFrame(columns=[\"author\", \"commits\"])\n",
    "\n",
    "# PRs: user login\n",
    "if \"user.login\" in df_pulls.columns:\n",
    "    pr_counts = df_pulls[\"user.login\"].dropna().value_counts().reset_index()\n",
    "    pr_counts.columns = [\"author\", \"prs\"]\n",
    "else:\n",
    "    pr_counts = pd.DataFrame(columns=[\"author\", \"prs\"])\n",
    "\n",
    "# PR Comments: user login\n",
    "if \"user.login\" in df_pr_comments.columns:\n",
    "    comment_counts = df_pr_comments[\"user.login\"].dropna().value_counts().reset_index()\n",
    "    comment_counts.columns = [\"author\", \"comments\"]\n",
    "else:\n",
    "    comment_counts = pd.DataFrame(columns=[\"author\", \"comments\"])\n",
    "\n",
    "# PR Reviews: user login\n",
    "if \"user.login\" in df_reviews.columns:\n",
    "    review_counts = df_reviews[\"user.login\"].dropna().value_counts().reset_index()\n",
    "    review_counts.columns = [\"author\", \"reviews\"]\n",
    "else:\n",
    "    review_counts = pd.DataFrame(columns=[\"author\", \"reviews\"])\n",
    "\n",
    "print(\"Component counts extracted:\")\n",
    "print(f\" {len(commit_counts):,} unique commit authors\")\n",
    "print(f\" {len(pr_counts):,} unique PR authors\")\n",
    "print(f\" {len(comment_counts):,} unique commenters\")\n",
    "print(f\" {len(review_counts):,} unique reviewers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8be8b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327 unique contributors identified\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>commits</th>\n",
       "      <th>prs</th>\n",
       "      <th>comments</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jscheffl</td>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amoghrajesh</td>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>potiuk</td>\n",
       "      <td>56</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dependabot[bot]</td>\n",
       "      <td>39</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pierrejeambrun</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  commits  prs  comments  reviews\n",
       "0         jscheffl       59   37         0      227\n",
       "1      amoghrajesh       59   31         0      140\n",
       "2           potiuk       56   38         0      171\n",
       "3  dependabot[bot]       39   68         0        0\n",
       "4   pierrejeambrun       33   31         0       82"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Merge all counts via outer join\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [commit_counts, pr_counts, comment_counts, review_counts]\n",
    "dfs = [df for df in dfs if not df.empty]\n",
    "\n",
    "if dfs:\n",
    "    df_contributors = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=\"author\", how=\"outer\"),\n",
    "        dfs\n",
    "    )\n",
    "else:\n",
    "    df_contributors = pd.DataFrame(columns=[\"author\"])\n",
    "\n",
    "# Fill missing counts with 0\n",
    "for col in [\"commits\", \"prs\", \"comments\", \"reviews\"]:\n",
    "    if col not in df_contributors.columns:\n",
    "        df_contributors[col] = 0\n",
    "    else:\n",
    "        df_contributors[col] = df_contributors[col].fillna(0).astype(int)\n",
    "\n",
    "print(f\"{len(df_contributors):,} unique contributors identified\")\n",
    "df_contributors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b7d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scoring and ranking complete\n",
      "  Columns: ['author', 'commits', 'prs', 'comments', 'reviews', 'score', 'tier', 'overall_rank', 'tier_rank', 'percentile']\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute score, tier, ranks, and percentile\n",
    "\n",
    "# Score: commits×5 + prs×10 + comments×2 + reviews×3, capped at 100\n",
    "df_contributors[\"raw_score\"] = (\n",
    "    df_contributors[\"commits\"] * 5\n",
    "    + df_contributors[\"prs\"] * 10\n",
    "    + df_contributors[\"comments\"] * 2\n",
    "    + df_contributors[\"reviews\"] * 3\n",
    ")\n",
    "df_contributors[\"score\"] = df_contributors[\"raw_score\"].clip(upper=100)\n",
    "\n",
    "# Tier assignment based on commits + prs activity level\n",
    "def assign_tier(row):\n",
    "    activity = row[\"commits\"] + row[\"prs\"]\n",
    "    if activity >= 20:\n",
    "        return \"core\"\n",
    "    elif activity >= 5:\n",
    "        return \"active\"\n",
    "    elif activity >= 1:\n",
    "        return \"contributor\"\n",
    "    else:\n",
    "        return \"observer\"\n",
    "\n",
    "df_contributors[\"tier\"] = df_contributors.apply(assign_tier, axis=1)\n",
    "\n",
    "# Tier ordering for proper sorting\n",
    "tier_order = {\"core\": 0, \"active\": 1, \"contributor\": 2, \"observer\": 3}\n",
    "df_contributors[\"tier_order\"] = df_contributors[\"tier\"].map(tier_order)\n",
    "\n",
    "# Overall rank (by score descending, ties get same rank)\n",
    "df_contributors = df_contributors.sort_values(\"score\", ascending=False)\n",
    "df_contributors[\"overall_rank\"] = df_contributors[\"score\"].rank(method=\"min\", ascending=False).astype(int)\n",
    "\n",
    "# Tier rank (by score descending within each tier)\n",
    "df_contributors[\"tier_rank\"] = (\n",
    "    df_contributors.groupby(\"tier\")[\"score\"]\n",
    "    .rank(method=\"min\", ascending=False)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Percentile (0-100, where 100 = top contributor)\n",
    "df_contributors[\"percentile\"] = (\n",
    "    df_contributors[\"score\"].rank(pct=True) * 100\n",
    ").round(2)\n",
    "\n",
    "# Clean up and reorder columns\n",
    "df_contributors = df_contributors.drop(columns=[\"raw_score\", \"tier_order\"])\n",
    "df_contributors = df_contributors[\n",
    "    [\"author\", \"commits\", \"prs\", \"comments\", \"reviews\", \"score\", \"tier\", \"overall_rank\", \"tier_rank\", \"percentile\"]\n",
    "]\n",
    "\n",
    "print(\" Scoring and ranking complete\")\n",
    "print(f\"  Columns: {list(df_contributors.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f9728",
   "metadata": {},
   "source": [
    "### Task 2 Output: Top Contributors, Tier Distribution, Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c4e4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 CONTRIBUTORS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>commits</th>\n",
       "      <th>prs</th>\n",
       "      <th>comments</th>\n",
       "      <th>reviews</th>\n",
       "      <th>score</th>\n",
       "      <th>tier</th>\n",
       "      <th>overall_rank</th>\n",
       "      <th>tier_rank</th>\n",
       "      <th>percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jscheffl</td>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>100</td>\n",
       "      <td>core</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ferruzzi</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Prab-27</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dabla</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>100</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amoghrajesh</td>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>100</td>\n",
       "      <td>core</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>KamranImaaz</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>core</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Crowiant</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SameerMesiah97</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>100</td>\n",
       "      <td>core</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>github-actions[bot]</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>core</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>mistercrunch</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>observer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  commits  prs  comments  reviews  score      tier  \\\n",
       "0               jscheffl       59   37         0      227    100      core   \n",
       "53              ferruzzi        4    3         0       40    100    active   \n",
       "26               Prab-27       10    9         0       17    100    active   \n",
       "27                 dabla       10    9         0       31    100    active   \n",
       "1            amoghrajesh       59   31         0      140    100      core   \n",
       "29           KamranImaaz        9   12         0       13    100      core   \n",
       "30              Crowiant        9    6         0        0    100    active   \n",
       "31        SameerMesiah97        8   12         0       57    100      core   \n",
       "193  github-actions[bot]        0   99         0        0    100      core   \n",
       "259         mistercrunch        0    0       271        0    100  observer   \n",
       "\n",
       "     overall_rank  tier_rank  percentile  \n",
       "0               1          1       92.81  \n",
       "53              1          1       92.81  \n",
       "26              1          1       92.81  \n",
       "27              1          1       92.81  \n",
       "1               1          1       92.81  \n",
       "29              1          1       92.81  \n",
       "30              1          1       92.81  \n",
       "31              1          1       92.81  \n",
       "193             1          1       92.81  \n",
       "259             1          1       92.81  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output 1: Top 10 Contributors \n",
    "\n",
    "print(\"TOP 10 CONTRIBUTORS\")\n",
    "\n",
    "top_10 = df_contributors.head(10)\n",
    "display(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5429c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIER DISTRIBUTION\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tier</th>\n",
       "      <th>contributor_count</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>total_commits</th>\n",
       "      <th>total_prs</th>\n",
       "      <th>pct_of_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contributor</td>\n",
       "      <td>183</td>\n",
       "      <td>16.02</td>\n",
       "      <td>153</td>\n",
       "      <td>159</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>observer</td>\n",
       "      <td>68</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>active</td>\n",
       "      <td>51</td>\n",
       "      <td>75.84</td>\n",
       "      <td>252</td>\n",
       "      <td>241</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>core</td>\n",
       "      <td>25</td>\n",
       "      <td>100.00</td>\n",
       "      <td>595</td>\n",
       "      <td>600</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tier  contributor_count  avg_score  total_commits  total_prs  \\\n",
       "1  contributor                183      16.02            153        159   \n",
       "3     observer                 68      16.78              0          0   \n",
       "0       active                 51      75.84            252        241   \n",
       "2         core                 25     100.00            595        600   \n",
       "\n",
       "   pct_of_total  \n",
       "1          56.0  \n",
       "3          20.8  \n",
       "0          15.6  \n",
       "2           7.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output 2: Tier Distribution\n",
    "\n",
    "print(\"TIER DISTRIBUTION\")\n",
    "\n",
    "tier_dist = (\n",
    "    df_contributors.groupby(\"tier\", as_index=False)\n",
    "    .agg(\n",
    "        contributor_count=(\"author\", \"count\"),\n",
    "        avg_score=(\"score\", \"mean\"),\n",
    "        total_commits=(\"commits\", \"sum\"),\n",
    "        total_prs=(\"prs\", \"sum\"),\n",
    "    )\n",
    "    .sort_values(\"contributor_count\", ascending=False)\n",
    ")\n",
    "tier_dist[\"avg_score\"] = tier_dist[\"avg_score\"].round(2)\n",
    "tier_dist[\"pct_of_total\"] = (tier_dist[\"contributor_count\"] / len(df_contributors) * 100).round(1)\n",
    "display(tier_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62f77684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY STATISTICS\n",
      "  Total Contributors                327\n",
      "  Total Commits                    1000\n",
      "  Total PRs                        1000\n",
      "  Total Comments                   1000\n",
      "  Total Reviews                    2304\n",
      "  Avg Score                       31.93\n",
      "  Median Score                     15.0\n",
      "  Max Score                         100\n",
      "  Core Contributors                  25\n",
      "  Active Contributors                51\n"
     ]
    }
   ],
   "source": [
    "# Output 3: Summary Statistics \n",
    "print(\"SUMMARY STATISTICS\")\n",
    "\n",
    "summary_stats = {\n",
    "    \"Total Contributors\": len(df_contributors),\n",
    "    \"Total Commits\": df_contributors[\"commits\"].sum(),\n",
    "    \"Total PRs\": df_contributors[\"prs\"].sum(),\n",
    "    \"Total Comments\": df_contributors[\"comments\"].sum(),\n",
    "    \"Total Reviews\": df_contributors[\"reviews\"].sum(),\n",
    "    \"Avg Score\": round(df_contributors[\"score\"].mean(), 2),\n",
    "    \"Median Score\": df_contributors[\"score\"].median(),\n",
    "    \"Max Score\": df_contributors[\"score\"].max(),\n",
    "    \"Core Contributors\": len(df_contributors[df_contributors[\"tier\"] == \"core\"]),\n",
    "    \"Active Contributors\": len(df_contributors[df_contributors[\"tier\"] == \"active\"]),\n",
    "}\n",
    "\n",
    "for k, v in summary_stats.items():\n",
    "    print(f\"  {k:25s}  {v:>10,}\" if isinstance(v, int) else f\"  {k:25s}  {v:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba965f0d",
   "metadata": {},
   "source": [
    "## Task 3: Export to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca06614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Worksheet 2: Part2_Contributors\n",
    "\n",
    "WS2_TITLE = \"Part2_Contributors\"\n",
    "ws2_id = ensure_worksheet(WS2_TITLE)\n",
    "\n",
    "values = df_to_sheet_values(df_contributors)\n",
    "num_rows = len(values)\n",
    "num_cols = len(values[0])\n",
    "\n",
    "# Write data\n",
    "sheets_api.values().update(\n",
    "    spreadsheetId=SPREADSHEET_ID,\n",
    "    range=f\"'{WS2_TITLE}'!A1\",\n",
    "    valueInputOption=\"RAW\",\n",
    "    body={\"values\": values},\n",
    ").execute()\n",
    "\n",
    "# Format header\n",
    "format_header(ws2_id, num_cols)\n",
    "\n",
    "print(f\"\\nPart2_Contributors: {num_rows:,} rows × {num_cols} columns\")\n",
    "\n",
    "print(\"  Task 3 Complete - Google Sheets Export\")\n",
    "\n",
    "print(f\"\\n  Spreadsheet: {spreadsheet_url}\")\n",
    "print(f\"  Worksheets:\")\n",
    "print(f\" Part1_Analytics  - Queries 4a–4e (labeled sections)\")\n",
    "print(f\" Part2_Contributors - {len(df_contributors):,} contributors\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
